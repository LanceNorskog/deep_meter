{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Meter_ARPAbet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "cell_type": "markdown",
      "source": [
        "# [Keras + Universal Sentence Encoder = Deep Meter] (https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/) "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eAVQGidpL8v5"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook creates an autoencoder using the Universal Sentence Encoder. The autoencoder output is CMUdict syllables. The dataset is that subset of Allison Parrish's Project Gutenberg poetry archive which happens to scan in iambic pentameter.\n",
        "\n",
        "The notebook is based on Chengwei Zhang's example of wrapping the USE inside a larger tensorflow model saves to a Keras model (without save the USE itself in the TF model).\n",
        "\n",
        "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n",
        "\n",
        "Since there are 10 one-hot values for 10 sets of 6k syllables, this is \"multi-label classification\"\n",
        "Changes for multi-label classification:\n",
        "sigmoid activation instead of softmax\n",
        "binary_crossentropy\n",
        "\n",
        "Text format is tab-separated, 2 columns: first text, second multi-level\n",
        "array of syllables:\n",
        "\n",
        "Multi-output version\n",
        "\n",
        "Use ARPAbet directly instead of syllables"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "outputId": "430054fd-f545-44ee-aad2-8f15735b8b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "#!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "#!pip3 install --quiet tensorflow-hub\n",
        "#%cd /content\n",
        "!git clone https://github.com/LanceNorskog/deep_meter || true\n",
        "%cd /content/deep_meter\n",
        "!git pull\n",
        "# could not figure out how to read gzipped files as text!\n",
        "!gunzip -qf blobs/*.gz || true\n",
        "!gunzip -qf prepped_data/*.gz || true"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep_meter'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/23)   \u001b[K\rremote: Counting objects:   8% (2/23)   \u001b[K\rremote: Counting objects:  13% (3/23)   \u001b[K\rremote: Counting objects:  17% (4/23)   \u001b[K\rremote: Counting objects:  21% (5/23)   \u001b[K\rremote: Counting objects:  26% (6/23)   \u001b[K\rremote: Counting objects:  30% (7/23)   \u001b[K\rremote: Counting objects:  34% (8/23)   \u001b[K\rremote: Counting objects:  39% (9/23)   \u001b[K\rremote: Counting objects:  43% (10/23)   \u001b[K\rremote: Counting objects:  47% (11/23)   \u001b[K\rremote: Counting objects:  52% (12/23)   \u001b[K\rremote: Counting objects:  56% (13/23)   \u001b[K\rremote: Counting objects:  60% (14/23)   \u001b[K\rremote: Counting objects:  65% (15/23)   \u001b[K\rremote: Counting objects:  69% (16/23)   \u001b[K\rremote: Counting objects:  73% (17/23)   \u001b[K\rremote: Counting objects:  78% (18/23)   \u001b[K\rremote: Counting objects:  82% (19/23)   \u001b[K\rremote: Counting objects:  86% (20/23)   \u001b[K\rremote: Counting objects:  91% (21/23)   \u001b[K\rremote: Counting objects:  95% (22/23)   \u001b[K\rremote: Counting objects: 100% (23/23)   \u001b[K\rremote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/18)   \u001b[K\rremote: Compressing objects:  11% (2/18)   \u001b[K\rremote: Compressing objects:  16% (3/18)   \u001b[K\rremote: Compressing objects:  22% (4/18)   \u001b[K\rremote: Compressing objects:  27% (5/18)   \u001b[K\rremote: Compressing objects:  33% (6/18)   \u001b[K\rremote: Compressing objects:  38% (7/18)   \u001b[K\rremote: Compressing objects:  44% (8/18)   \u001b[K\rremote: Compressing objects:  50% (9/18)   \u001b[K\rremote: Compressing objects:  55% (10/18)   \u001b[K\rremote: Compressing objects:  61% (11/18)   \u001b[K\rremote: Compressing objects:  66% (12/18)   \u001b[K\rremote: Compressing objects:  72% (13/18)   \u001b[K\rremote: Compressing objects:  77% (14/18)   \u001b[K\rremote: Compressing objects:  83% (15/18)   \u001b[K\rremote: Compressing objects:  88% (16/18)   \u001b[K\rremote: Compressing objects:  94% (17/18)   \u001b[K\rremote: Compressing objects: 100% (18/18)   \u001b[K\rremote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "Receiving objects:   0% (1/266)   \rReceiving objects:   1% (3/266)   \rReceiving objects:   2% (6/266)   \rReceiving objects:   3% (8/266)   \rReceiving objects:   4% (11/266)   \rReceiving objects:   5% (14/266)   \rReceiving objects:   6% (16/266)   \rReceiving objects:   7% (19/266)   \rReceiving objects:   8% (22/266)   \rReceiving objects:   9% (24/266)   \rReceiving objects:  10% (27/266)   \rReceiving objects:  11% (30/266)   \rReceiving objects:  12% (32/266)   \rReceiving objects:  13% (35/266)   \rReceiving objects:  14% (38/266)   \rReceiving objects:  15% (40/266)   \rReceiving objects:  16% (43/266)   \rReceiving objects:  17% (46/266)   \rReceiving objects:  18% (48/266)   \rReceiving objects:  19% (51/266)   \rReceiving objects:  20% (54/266)   \rReceiving objects:  21% (56/266)   \rReceiving objects:  22% (59/266)   \rReceiving objects:  23% (62/266)   \rReceiving objects:  24% (64/266)   \rReceiving objects:  25% (67/266)   \rReceiving objects:  26% (70/266)   \rReceiving objects:  27% (72/266)   \rReceiving objects:  28% (75/266)   \rReceiving objects:  29% (78/266)   \rReceiving objects:  30% (80/266)   \rReceiving objects:  31% (83/266)   \rReceiving objects:  32% (86/266)   \rReceiving objects:  33% (88/266)   \rReceiving objects:  34% (91/266)   \rReceiving objects:  35% (94/266)   \rReceiving objects:  36% (96/266)   \rReceiving objects:  37% (99/266)   \rReceiving objects:  38% (102/266)   \rReceiving objects:  39% (104/266)   \rReceiving objects:  40% (107/266)   \rReceiving objects:  41% (110/266)   \rReceiving objects:  42% (112/266)   \rReceiving objects:  43% (115/266)   \rReceiving objects:  44% (118/266)   \rReceiving objects:  45% (120/266)   \rReceiving objects:  46% (123/266)   \rReceiving objects:  47% (126/266)   \rReceiving objects:  48% (128/266)   \rReceiving objects:  49% (131/266)   \rReceiving objects:  50% (133/266)   \rReceiving objects:  51% (136/266)   \rReceiving objects:  52% (139/266)   \rReceiving objects:  53% (141/266)   \rReceiving objects:  54% (144/266)   \rReceiving objects:  55% (147/266)   \rReceiving objects:  56% (149/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  57% (152/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  58% (155/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  59% (157/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  60% (160/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  61% (163/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  62% (165/266), 12.74 MiB | 25.47 MiB/s   \rremote: Total 266 (delta 11), reused 16 (delta 5), pack-reused 243\u001b[K\n",
            "Receiving objects:  63% (168/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  64% (171/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  65% (173/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  66% (176/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  67% (179/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  68% (181/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  69% (184/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  70% (187/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  71% (189/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  72% (192/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  73% (195/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  74% (197/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  75% (200/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  76% (203/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  77% (205/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  78% (208/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  79% (211/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  80% (213/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  81% (216/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  82% (219/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  83% (221/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  84% (224/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  85% (227/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  86% (229/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  87% (232/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  88% (235/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  89% (237/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  90% (240/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  91% (243/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  92% (245/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  93% (248/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  94% (251/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  95% (253/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  96% (256/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  97% (259/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  98% (261/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects:  99% (264/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects: 100% (266/266), 12.74 MiB | 25.47 MiB/s   \rReceiving objects: 100% (266/266), 20.79 MiB | 26.32 MiB/s, done.\n",
            "Resolving deltas:   0% (0/142)   \rResolving deltas:  26% (38/142)   \rResolving deltas:  28% (40/142)   \rResolving deltas:  40% (58/142)   \rResolving deltas:  47% (68/142)   \rResolving deltas:  50% (72/142)   \rResolving deltas:  59% (85/142)   \rResolving deltas:  61% (88/142)   \rResolving deltas:  62% (89/142)   \rResolving deltas:  63% (90/142)   \rResolving deltas:  69% (98/142)   \rResolving deltas:  72% (103/142)   \rResolving deltas:  76% (109/142)   \rResolving deltas:  80% (114/142)   \rResolving deltas:  81% (116/142)   \rResolving deltas:  82% (117/142)   \rResolving deltas:  85% (122/142)   \rResolving deltas:  89% (127/142)   \rResolving deltas:  91% (130/142)   \rResolving deltas:  95% (136/142)   \rResolving deltas:  96% (137/142)   \rResolving deltas:  97% (139/142)   \rResolving deltas: 100% (142/142)   \rResolving deltas: 100% (142/142), done.\n",
            "/content/deep_meter\n",
            "Already up to date.\n",
            "gzip: blobs/*.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha",
        "outputId": "879e8d8f-6c0c-4ab9-cc60-63ffe76c7e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# boilerplate from base notebook\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Nadam, Adam\n",
        "np.random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "feBc_8Y-pt6F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# github deep_meter code\n",
        "import utils\n",
        "# should not need this to use utils.flatten but is true anyway?\n",
        "from itertools import chain\n",
        "import subprocess\n",
        "import arpabets\n",
        "# misc for this notebook\n",
        "from ast import literal_eval\n",
        "\n",
        "import scipy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zwty8Z6mAkdV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q8F4LNGFqOiq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "#embed = hub.Module(module_url)\n",
        "#embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FwAQNy1eMDkQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read classified poetry lines: text tab [['syll', 'la', 'ble'], ...]\n",
        "# clip to only most common syllables with syllable manager\n",
        "# ['words', ...], [[[0,0,1,0], ...]]\n",
        "def get_data(filename, arpabet_mgr, num_symbols):\n",
        "    stop_arpabet = 0\n",
        "    num_arpabets = arpabet_mgr.get_size()      \n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    num_lines = len(lines)\n",
        "    num_lines = 40000\n",
        "    text_lines = []\n",
        "    text_arpabets = []\n",
        "    for i in range(0, len(lines)):\n",
        "      if i == num_lines:\n",
        "        break\n",
        "      parts = lines[i].split(\"\\t\")\n",
        "      syllables = literal_eval(parts[1])\n",
        "      #print(syllables)\n",
        "      arpas = []\n",
        "      for s in syllables:\n",
        "        for p in s:\n",
        "          for x in p.split(' '):\n",
        "            arpas.append(x)\n",
        "      #print(arpas)\n",
        "      if len(arpas) < num_symbols:\n",
        "        text_lines.append(str(parts[0]))\n",
        "        text_arpabets.append(arpas)\n",
        "    num_lines = len(text_lines)\n",
        "    label_array = np.zeros((num_symbols, num_lines, num_arpabets), dtype=np.int8)\n",
        "    for i in range(0, num_lines):\n",
        "      for j in range(num_symbols):\n",
        "        label_array[j][i][stop_arpabet] = 1\n",
        "        # variable-length list of syllables\n",
        "        if j < len(text_arpabets[i]):\n",
        "          enc = arpabet_mgr.get_encoding(text_arpabets[i][j])\n",
        "          if enc >= 0 and enc < num_arpabets:\n",
        "            label_array[j][i][enc] = 1\n",
        "            label_array[j][i][stop_arpabet] = 0\n",
        "\n",
        "    return (text_lines, label_array)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3HAtd4X5DayF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# arpabets in descending order of occurrence - \n",
        "# ARPAbet phonemes + stop + pause\n",
        "# iambic pentameter\n",
        "meter_syllables = 10\n",
        "num_symbols = 4 * meter_syllables\n",
        "arpabets_mgr = arpabets.arpabets()\n",
        "num_arpabets = arpabets_mgr.get_size() \n",
        "#arpabets_weights = {}\n",
        "#counts = arpabets_mgr.get_counts()\n",
        "#maxim = np.max(counts)\n",
        "#for i in range(len(counts)):\n",
        "#  if counts[i] > 0:\n",
        "#    arpabets_weights[i] = 1/(counts[i]/maxim)\n",
        "#  else:\n",
        "#    arpabets_weights[i] = 0\n",
        "\n",
        "#print(arpabets_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5s7nAFdswrFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eN9aqig-QpDZ",
        "outputId": "2c9b9643-d3b9-4d42-e1f9-76e8db33b7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "(train_text, train_label) = get_data('prepped_data/gutenberg.iambic_pentameter.train', arpabets_mgr, num_symbols)\n",
        "print(len(train_text))\n",
        "print(train_label.shape)\n",
        "\n",
        "(test_text, test_label) = get_data('prepped_data/gutenberg.iambic_pentameter.test', arpabets_mgr, num_symbols)\n",
        "print(len(test_text))\n",
        "print(test_label.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000\n",
            "(40, 40000, 41)\n",
            "4474\n",
            "(40, 4474, 41)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Sf9A4Xl6J7c6"
      },
      "cell_type": "markdown",
      "source": [
        "## Embed training & test text"
      ]
    },
    {
      "metadata": {
        "id": "mbJCx9vzwrF9",
        "colab_type": "code",
        "outputId": "38caeefa-03a7-436c-f99d-30de87fb464a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "# important?\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "print(type(train_text))\n",
        "#train_text_t = tf.convert_to_tensor(train_text, dtype='string', name='training_text')\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  train_embeddings = session.run(embed(train_text))\n",
        "  test_embeddings = session.run(embed(test_text))\n",
        "train_text_d = np.array(train_embeddings)\n",
        "test_text_d = np.array(test_embeddings)\n",
        "print(train_text_d.shape)\n",
        "print(test_text_d.shape)\n",
        "# conserve space\n",
        "embed = None\n",
        "train_text = None\n",
        "train_embeddings = None"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "<class 'list'>\n",
            "(40000, 512)\n",
            "(4474, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-mQUV_7dIm-9",
        "outputId": "840acfb4-e11f-4a48-94db-b3afbf662834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_text_d.shape)\n",
        "# slow\n",
        "num_epochs = 40\n",
        "adam_0001 = Adam(0.0001)\n",
        "adam_0001 = tf.contrib.opt.NadamOptimizer(0.0001)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4474, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qX2rBOuxDP1m"
      },
      "cell_type": "markdown",
      "source": [
        "## Assemble model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "t3fllZkVjXKV",
        "outputId": "d0b13981-8a0e-45d3-95b4-dfdd71cabbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1598
        }
      },
      "cell_type": "code",
      "source": [
        "input_embeddings = layers.Input(shape=(512,), dtype=tf.float32, name='Input')\n",
        "dropout_input = layers.Dropout(0.2)(input_embeddings)\n",
        "dense = layers.Dense(1024, activation='relu', name='Convoluted')(dropout_input)\n",
        "dense = layers.Dropout(0.2)(input_embeddings)\n",
        "dense = layers.Dense(2048, activation='relu', name='Midway')(dense)\n",
        "dense = layers.Dropout(0.2)(input_embeddings)\n",
        "dense = layers.Dense(4096, activation='relu', name='Smooth')(dense)\n",
        "pred_array = []\n",
        "loss_array = []\n",
        "names_array = []\n",
        "for i in range(num_symbols):\n",
        "  name = 'Flatout'+\"{:0>2d}\".format(i)\n",
        "  pred_array.append(layers.Dense(num_arpabets, activation='softmax', name=name)(dense))\n",
        "  loss_array.append('categorical_crossentropy')\n",
        "  names_array.append(name)\n",
        "model = Model(inputs=input_embeddings, outputs=pred_array)\n",
        "model.compile(loss=loss_array, \n",
        "              optimizer=adam_0001, \n",
        "              metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input (InputLayer)              (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           Input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Smooth (Dense)                  (None, 4096)         2101248     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Flatout00 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout01 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout02 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout03 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout04 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout05 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout06 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout07 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout08 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout09 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout10 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout11 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout12 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout13 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout14 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout15 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout16 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout17 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout18 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout19 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout20 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout21 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout22 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout23 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout24 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout25 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout26 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout27 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout28 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout29 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout30 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout31 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout32 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout33 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout34 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout35 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout36 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout37 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout38 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Flatout39 (Dense)               (None, 41)           167977      Smooth[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 8,820,328\n",
            "Trainable params: 8,820,328\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bqcRy_JWXe0u"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Keras model and save weights\n",
        "This only trains and save our Keras layers not the embed module' weights."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_stfC_7VFhS8",
        "outputId": "6ce65246-79bc-48df-a618-794476ed57e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "use_saved_model=False\n",
        "\n",
        "print(train_label.shape)\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    train_labels = []\n",
        "    test_labels = []\n",
        "    for i in range(num_symbols):\n",
        "        train_labels.append(np.array(train_label[i]))\n",
        "        test_labels.append(np.array(test_label[i]))\n",
        "    history = model.fit(train_text_d, \n",
        "            train_labels,\n",
        "            validation_data=(test_text_d, test_labels),\n",
        "            epochs=num_epochs,\n",
        "            callbacks = [EarlyStopping(patience=2)],\n",
        "            batch_size=32,\n",
        "            #class_weight=syll_weights,\n",
        "            verbose=2\n",
        "            )\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 40000, 41)\n",
            "Train on 40000 samples, validate on 4474 samples\n",
            "Epoch 1/40\n",
            " - 143s - loss: 93.7128 - Flatout00_loss: 1.9088 - Flatout01_loss: 2.1471 - Flatout02_loss: 2.7411 - Flatout03_loss: 3.3937 - Flatout04_loss: 3.3316 - Flatout05_loss: 3.1916 - Flatout06_loss: 3.1494 - Flatout07_loss: 3.3135 - Flatout08_loss: 3.4115 - Flatout09_loss: 3.4135 - Flatout10_loss: 3.3576 - Flatout11_loss: 3.2895 - Flatout12_loss: 3.2968 - Flatout13_loss: 3.3406 - Flatout14_loss: 3.3651 - Flatout15_loss: 3.3469 - Flatout16_loss: 3.3157 - Flatout17_loss: 3.3099 - Flatout18_loss: 3.3355 - Flatout19_loss: 3.3539 - Flatout20_loss: 3.3613 - Flatout21_loss: 3.3663 - Flatout22_loss: 3.3826 - Flatout23_loss: 3.3765 - Flatout24_loss: 3.3080 - Flatout25_loss: 3.1071 - Flatout26_loss: 2.7217 - Flatout27_loss: 2.2089 - Flatout28_loss: 1.6601 - Flatout29_loss: 1.1540 - Flatout30_loss: 0.7586 - Flatout31_loss: 0.4929 - Flatout32_loss: 0.3229 - Flatout33_loss: 0.2249 - Flatout34_loss: 0.1838 - Flatout35_loss: 0.1581 - Flatout36_loss: 0.1544 - Flatout37_loss: 0.1538 - Flatout38_loss: 0.1542 - Flatout39_loss: 0.1493 - Flatout00_categorical_accuracy: 0.4404 - Flatout01_categorical_accuracy: 0.4134 - Flatout02_categorical_accuracy: 0.2813 - Flatout03_categorical_accuracy: 0.0755 - Flatout04_categorical_accuracy: 0.0880 - Flatout05_categorical_accuracy: 0.1298 - Flatout06_categorical_accuracy: 0.1613 - Flatout07_categorical_accuracy: 0.1106 - Flatout08_categorical_accuracy: 0.0672 - Flatout09_categorical_accuracy: 0.0701 - Flatout10_categorical_accuracy: 0.0968 - Flatout11_categorical_accuracy: 0.1234 - Flatout12_categorical_accuracy: 0.1140 - Flatout13_categorical_accuracy: 0.0984 - Flatout14_categorical_accuracy: 0.0853 - Flatout15_categorical_accuracy: 0.0938 - Flatout16_categorical_accuracy: 0.1105 - Flatout17_categorical_accuracy: 0.1163 - Flatout18_categorical_accuracy: 0.1055 - Flatout19_categorical_accuracy: 0.0990 - Flatout20_categorical_accuracy: 0.0952 - Flatout21_categorical_accuracy: 0.0973 - Flatout22_categorical_accuracy: 0.0936 - Flatout23_categorical_accuracy: 0.0909 - Flatout24_categorical_accuracy: 0.1324 - Flatout25_categorical_accuracy: 0.2386 - Flatout26_categorical_accuracy: 0.3871 - Flatout27_categorical_accuracy: 0.5461 - Flatout28_categorical_accuracy: 0.6954 - Flatout29_categorical_accuracy: 0.8166 - Flatout30_categorical_accuracy: 0.8990 - Flatout31_categorical_accuracy: 0.9479 - Flatout32_categorical_accuracy: 0.9751 - Flatout33_categorical_accuracy: 0.9890 - Flatout34_categorical_accuracy: 0.9945 - Flatout35_categorical_accuracy: 0.9979 - Flatout36_categorical_accuracy: 0.9980 - Flatout37_categorical_accuracy: 0.9977 - Flatout38_categorical_accuracy: 0.9975 - Flatout39_categorical_accuracy: 0.9982 - val_loss: 90.0732 - val_Flatout00_loss: 1.5218 - val_Flatout01_loss: 1.8532 - val_Flatout02_loss: 2.5080 - val_Flatout03_loss: 3.3247 - val_Flatout04_loss: 3.2684 - val_Flatout05_loss: 3.1458 - val_Flatout06_loss: 3.0873 - val_Flatout07_loss: 3.2719 - val_Flatout08_loss: 3.3799 - val_Flatout09_loss: 3.3796 - val_Flatout10_loss: 3.3316 - val_Flatout11_loss: 3.2549 - val_Flatout12_loss: 3.2932 - val_Flatout13_loss: 3.3241 - val_Flatout14_loss: 3.3337 - val_Flatout15_loss: 3.3033 - val_Flatout16_loss: 3.2893 - val_Flatout17_loss: 3.3019 - val_Flatout18_loss: 3.3252 - val_Flatout19_loss: 3.3479 - val_Flatout20_loss: 3.3442 - val_Flatout21_loss: 3.3425 - val_Flatout22_loss: 3.3439 - val_Flatout23_loss: 3.3483 - val_Flatout24_loss: 3.2701 - val_Flatout25_loss: 3.0446 - val_Flatout26_loss: 2.6167 - val_Flatout27_loss: 2.0961 - val_Flatout28_loss: 1.5211 - val_Flatout29_loss: 0.9926 - val_Flatout30_loss: 0.6097 - val_Flatout31_loss: 0.3340 - val_Flatout32_loss: 0.1755 - val_Flatout33_loss: 0.1024 - val_Flatout34_loss: 0.0438 - val_Flatout35_loss: 0.0162 - val_Flatout36_loss: 0.0104 - val_Flatout37_loss: 0.0103 - val_Flatout38_loss: 0.0037 - val_Flatout39_loss: 0.0012 - val_Flatout00_categorical_accuracy: 0.5371 - val_Flatout01_categorical_accuracy: 0.4931 - val_Flatout02_categorical_accuracy: 0.3404 - val_Flatout03_categorical_accuracy: 0.0878 - val_Flatout04_categorical_accuracy: 0.0903 - val_Flatout05_categorical_accuracy: 0.1498 - val_Flatout06_categorical_accuracy: 0.1721 - val_Flatout07_categorical_accuracy: 0.1120 - val_Flatout08_categorical_accuracy: 0.0831 - val_Flatout09_categorical_accuracy: 0.0744 - val_Flatout10_categorical_accuracy: 0.0988 - val_Flatout11_categorical_accuracy: 0.1234 - val_Flatout12_categorical_accuracy: 0.1109 - val_Flatout13_categorical_accuracy: 0.0992 - val_Flatout14_categorical_accuracy: 0.0878 - val_Flatout15_categorical_accuracy: 0.0919 - val_Flatout16_categorical_accuracy: 0.1169 - val_Flatout17_categorical_accuracy: 0.1160 - val_Flatout18_categorical_accuracy: 0.1035 - val_Flatout19_categorical_accuracy: 0.0865 - val_Flatout20_categorical_accuracy: 0.1042 - val_Flatout21_categorical_accuracy: 0.0975 - val_Flatout22_categorical_accuracy: 0.0910 - val_Flatout23_categorical_accuracy: 0.1039 - val_Flatout24_categorical_accuracy: 0.1426 - val_Flatout25_categorical_accuracy: 0.2374 - val_Flatout26_categorical_accuracy: 0.3941 - val_Flatout27_categorical_accuracy: 0.5512 - val_Flatout28_categorical_accuracy: 0.7041 - val_Flatout29_categorical_accuracy: 0.8214 - val_Flatout30_categorical_accuracy: 0.8999 - val_Flatout31_categorical_accuracy: 0.9513 - val_Flatout32_categorical_accuracy: 0.9765 - val_Flatout33_categorical_accuracy: 0.9873 - val_Flatout34_categorical_accuracy: 0.9953 - val_Flatout35_categorical_accuracy: 0.9984 - val_Flatout36_categorical_accuracy: 0.9991 - val_Flatout37_categorical_accuracy: 0.9991 - val_Flatout38_categorical_accuracy: 0.9998 - val_Flatout39_categorical_accuracy: 1.0000\n",
            "Epoch 2/40\n",
            " - 135s - loss: 89.3295 - Flatout00_loss: 1.4049 - Flatout01_loss: 1.7184 - Flatout02_loss: 2.4523 - Flatout03_loss: 3.3016 - Flatout04_loss: 3.2542 - Flatout05_loss: 3.1162 - Flatout06_loss: 3.0767 - Flatout07_loss: 3.2569 - Flatout08_loss: 3.3719 - Flatout09_loss: 3.3803 - Flatout10_loss: 3.3191 - Flatout11_loss: 3.2458 - Flatout12_loss: 3.2554 - Flatout13_loss: 3.3035 - Flatout14_loss: 3.3282 - Flatout15_loss: 3.3072 - Flatout16_loss: 3.2738 - Flatout17_loss: 3.2706 - Flatout18_loss: 3.2982 - Flatout19_loss: 3.3171 - Flatout20_loss: 3.3265 - Flatout21_loss: 3.3283 - Flatout22_loss: 3.3411 - Flatout23_loss: 3.3267 - Flatout24_loss: 3.2509 - Flatout25_loss: 3.0313 - Flatout26_loss: 2.6247 - Flatout27_loss: 2.0916 - Flatout28_loss: 1.5213 - Flatout29_loss: 1.0013 - Flatout30_loss: 0.6018 - Flatout31_loss: 0.3349 - Flatout32_loss: 0.1678 - Flatout33_loss: 0.0755 - Flatout34_loss: 0.0335 - Flatout35_loss: 0.0114 - Flatout36_loss: 0.0047 - Flatout37_loss: 0.0022 - Flatout38_loss: 9.3293e-04 - Flatout39_loss: 6.5665e-04 - Flatout00_categorical_accuracy: 0.5806 - Flatout01_categorical_accuracy: 0.5162 - Flatout02_categorical_accuracy: 0.3552 - Flatout03_categorical_accuracy: 0.0931 - Flatout04_categorical_accuracy: 0.0990 - Flatout05_categorical_accuracy: 0.1458 - Flatout06_categorical_accuracy: 0.1653 - Flatout07_categorical_accuracy: 0.1151 - Flatout08_categorical_accuracy: 0.0784 - Flatout09_categorical_accuracy: 0.0776 - Flatout10_categorical_accuracy: 0.1015 - Flatout11_categorical_accuracy: 0.1246 - Flatout12_categorical_accuracy: 0.1159 - Flatout13_categorical_accuracy: 0.1033 - Flatout14_categorical_accuracy: 0.0924 - Flatout15_categorical_accuracy: 0.0974 - Flatout16_categorical_accuracy: 0.1142 - Flatout17_categorical_accuracy: 0.1186 - Flatout18_categorical_accuracy: 0.1089 - Flatout19_categorical_accuracy: 0.1026 - Flatout20_categorical_accuracy: 0.0978 - Flatout21_categorical_accuracy: 0.0998 - Flatout22_categorical_accuracy: 0.0965 - Flatout23_categorical_accuracy: 0.1038 - Flatout24_categorical_accuracy: 0.1424 - Flatout25_categorical_accuracy: 0.2415 - Flatout26_categorical_accuracy: 0.3883 - Flatout27_categorical_accuracy: 0.5476 - Flatout28_categorical_accuracy: 0.6976 - Flatout29_categorical_accuracy: 0.8181 - Flatout30_categorical_accuracy: 0.9005 - Flatout31_categorical_accuracy: 0.9495 - Flatout32_categorical_accuracy: 0.9770 - Flatout33_categorical_accuracy: 0.9907 - Flatout34_categorical_accuracy: 0.9962 - Flatout35_categorical_accuracy: 0.9989 - Flatout36_categorical_accuracy: 0.9996 - Flatout37_categorical_accuracy: 0.9998 - Flatout38_categorical_accuracy: 1.0000 - Flatout39_categorical_accuracy: 1.0000 - val_loss: 88.9313 - val_Flatout00_loss: 1.2972 - val_Flatout01_loss: 1.6385 - val_Flatout02_loss: 2.3819 - val_Flatout03_loss: 3.2800 - val_Flatout04_loss: 3.2324 - val_Flatout05_loss: 3.1087 - val_Flatout06_loss: 3.0493 - val_Flatout07_loss: 3.2393 - val_Flatout08_loss: 3.3693 - val_Flatout09_loss: 3.3672 - val_Flatout10_loss: 3.3227 - val_Flatout11_loss: 3.2404 - val_Flatout12_loss: 3.2771 - val_Flatout13_loss: 3.3074 - val_Flatout14_loss: 3.3238 - val_Flatout15_loss: 3.2906 - val_Flatout16_loss: 3.2777 - val_Flatout17_loss: 3.2937 - val_Flatout18_loss: 3.3109 - val_Flatout19_loss: 3.3339 - val_Flatout20_loss: 3.3326 - val_Flatout21_loss: 3.3333 - val_Flatout22_loss: 3.3292 - val_Flatout23_loss: 3.3279 - val_Flatout24_loss: 3.2465 - val_Flatout25_loss: 3.0191 - val_Flatout26_loss: 2.5938 - val_Flatout27_loss: 2.0673 - val_Flatout28_loss: 1.4988 - val_Flatout29_loss: 0.9763 - val_Flatout30_loss: 0.5940 - val_Flatout31_loss: 0.3234 - val_Flatout32_loss: 0.1679 - val_Flatout33_loss: 0.0981 - val_Flatout34_loss: 0.0417 - val_Flatout35_loss: 0.0153 - val_Flatout36_loss: 0.0104 - val_Flatout37_loss: 0.0101 - val_Flatout38_loss: 0.0029 - val_Flatout39_loss: 2.9681e-04 - val_Flatout00_categorical_accuracy: 0.6109 - val_Flatout01_categorical_accuracy: 0.5438 - val_Flatout02_categorical_accuracy: 0.3724 - val_Flatout03_categorical_accuracy: 0.0997 - val_Flatout04_categorical_accuracy: 0.1053 - val_Flatout05_categorical_accuracy: 0.1500 - val_Flatout06_categorical_accuracy: 0.1726 - val_Flatout07_categorical_accuracy: 0.1129 - val_Flatout08_categorical_accuracy: 0.0749 - val_Flatout09_categorical_accuracy: 0.0767 - val_Flatout10_categorical_accuracy: 0.1010 - val_Flatout11_categorical_accuracy: 0.1249 - val_Flatout12_categorical_accuracy: 0.1124 - val_Flatout13_categorical_accuracy: 0.1044 - val_Flatout14_categorical_accuracy: 0.0869 - val_Flatout15_categorical_accuracy: 0.0966 - val_Flatout16_categorical_accuracy: 0.1196 - val_Flatout17_categorical_accuracy: 0.1171 - val_Flatout18_categorical_accuracy: 0.1039 - val_Flatout19_categorical_accuracy: 0.0970 - val_Flatout20_categorical_accuracy: 0.1008 - val_Flatout21_categorical_accuracy: 0.0975 - val_Flatout22_categorical_accuracy: 0.0914 - val_Flatout23_categorical_accuracy: 0.1075 - val_Flatout24_categorical_accuracy: 0.1430 - val_Flatout25_categorical_accuracy: 0.2385 - val_Flatout26_categorical_accuracy: 0.3943 - val_Flatout27_categorical_accuracy: 0.5512 - val_Flatout28_categorical_accuracy: 0.7041 - val_Flatout29_categorical_accuracy: 0.8214 - val_Flatout30_categorical_accuracy: 0.8999 - val_Flatout31_categorical_accuracy: 0.9513 - val_Flatout32_categorical_accuracy: 0.9765 - val_Flatout33_categorical_accuracy: 0.9873 - val_Flatout34_categorical_accuracy: 0.9953 - val_Flatout35_categorical_accuracy: 0.9984 - val_Flatout36_categorical_accuracy: 0.9991 - val_Flatout37_categorical_accuracy: 0.9991 - val_Flatout38_categorical_accuracy: 0.9998 - val_Flatout39_categorical_accuracy: 1.0000\n",
            "Epoch 3/40\n",
            " - 136s - loss: 88.4249 - Flatout00_loss: 1.2633 - Flatout01_loss: 1.5731 - Flatout02_loss: 2.3655 - Flatout03_loss: 3.2642 - Flatout04_loss: 3.2217 - Flatout05_loss: 3.0850 - Flatout06_loss: 3.0478 - Flatout07_loss: 3.2330 - Flatout08_loss: 3.3545 - Flatout09_loss: 3.3655 - Flatout10_loss: 3.3034 - Flatout11_loss: 3.2290 - Flatout12_loss: 3.2392 - Flatout13_loss: 3.2871 - Flatout14_loss: 3.3133 - Flatout15_loss: 3.2925 - Flatout16_loss: 3.2598 - Flatout17_loss: 3.2562 - Flatout18_loss: 3.2847 - Flatout19_loss: 3.3031 - Flatout20_loss: 3.3134 - Flatout21_loss: 3.3129 - Flatout22_loss: 3.3234 - Flatout23_loss: 3.3084 - Flatout24_loss: 3.2318 - Flatout25_loss: 3.0126 - Flatout26_loss: 2.6078 - Flatout27_loss: 2.0752 - Flatout28_loss: 1.5065 - Flatout29_loss: 0.9894 - Flatout30_loss: 0.5913 - Flatout31_loss: 0.3272 - Flatout32_loss: 0.1623 - Flatout33_loss: 0.0721 - Flatout34_loss: 0.0316 - Flatout35_loss: 0.0106 - Flatout36_loss: 0.0042 - Flatout37_loss: 0.0017 - Flatout38_loss: 4.7510e-04 - Flatout39_loss: 1.9763e-04 - Flatout00_categorical_accuracy: 0.6246 - Flatout01_categorical_accuracy: 0.5588 - Flatout02_categorical_accuracy: 0.3770 - Flatout03_categorical_accuracy: 0.1019 - Flatout04_categorical_accuracy: 0.1071 - Flatout05_categorical_accuracy: 0.1505 - Flatout06_categorical_accuracy: 0.1674 - Flatout07_categorical_accuracy: 0.1167 - Flatout08_categorical_accuracy: 0.0818 - Flatout09_categorical_accuracy: 0.0829 - Flatout10_categorical_accuracy: 0.1014 - Flatout11_categorical_accuracy: 0.1260 - Flatout12_categorical_accuracy: 0.1177 - Flatout13_categorical_accuracy: 0.1046 - Flatout14_categorical_accuracy: 0.0953 - Flatout15_categorical_accuracy: 0.1018 - Flatout16_categorical_accuracy: 0.1157 - Flatout17_categorical_accuracy: 0.1191 - Flatout18_categorical_accuracy: 0.1104 - Flatout19_categorical_accuracy: 0.1034 - Flatout20_categorical_accuracy: 0.0995 - Flatout21_categorical_accuracy: 0.1012 - Flatout22_categorical_accuracy: 0.0976 - Flatout23_categorical_accuracy: 0.1063 - Flatout24_categorical_accuracy: 0.1461 - Flatout25_categorical_accuracy: 0.2428 - Flatout26_categorical_accuracy: 0.3884 - Flatout27_categorical_accuracy: 0.5476 - Flatout28_categorical_accuracy: 0.6976 - Flatout29_categorical_accuracy: 0.8181 - Flatout30_categorical_accuracy: 0.9005 - Flatout31_categorical_accuracy: 0.9495 - Flatout32_categorical_accuracy: 0.9770 - Flatout33_categorical_accuracy: 0.9907 - Flatout34_categorical_accuracy: 0.9962 - Flatout35_categorical_accuracy: 0.9989 - Flatout36_categorical_accuracy: 0.9996 - Flatout37_categorical_accuracy: 0.9998 - Flatout38_categorical_accuracy: 1.0000 - Flatout39_categorical_accuracy: 1.0000 - val_loss: 88.3550 - val_Flatout00_loss: 1.1981 - val_Flatout01_loss: 1.5224 - val_Flatout02_loss: 2.3157 - val_Flatout03_loss: 3.2510 - val_Flatout04_loss: 3.2083 - val_Flatout05_loss: 3.0896 - val_Flatout06_loss: 3.0317 - val_Flatout07_loss: 3.2241 - val_Flatout08_loss: 3.3585 - val_Flatout09_loss: 3.3601 - val_Flatout10_loss: 3.3174 - val_Flatout11_loss: 3.2302 - val_Flatout12_loss: 3.2671 - val_Flatout13_loss: 3.2999 - val_Flatout14_loss: 3.3198 - val_Flatout15_loss: 3.2839 - val_Flatout16_loss: 3.2704 - val_Flatout17_loss: 3.2926 - val_Flatout18_loss: 3.3042 - val_Flatout19_loss: 3.3237 - val_Flatout20_loss: 3.3253 - val_Flatout21_loss: 3.3237 - val_Flatout22_loss: 3.3217 - val_Flatout23_loss: 3.3206 - val_Flatout24_loss: 3.2365 - val_Flatout25_loss: 3.0085 - val_Flatout26_loss: 2.5819 - val_Flatout27_loss: 2.0577 - val_Flatout28_loss: 1.4894 - val_Flatout29_loss: 0.9691 - val_Flatout30_loss: 0.5895 - val_Flatout31_loss: 0.3197 - val_Flatout32_loss: 0.1650 - val_Flatout33_loss: 0.0967 - val_Flatout34_loss: 0.0414 - val_Flatout35_loss: 0.0152 - val_Flatout36_loss: 0.0108 - val_Flatout37_loss: 0.0106 - val_Flatout38_loss: 0.0029 - val_Flatout39_loss: 1.1145e-04 - val_Flatout00_categorical_accuracy: 0.6453 - val_Flatout01_categorical_accuracy: 0.5827 - val_Flatout02_categorical_accuracy: 0.3923 - val_Flatout03_categorical_accuracy: 0.1071 - val_Flatout04_categorical_accuracy: 0.1127 - val_Flatout05_categorical_accuracy: 0.1529 - val_Flatout06_categorical_accuracy: 0.1737 - val_Flatout07_categorical_accuracy: 0.1147 - val_Flatout08_categorical_accuracy: 0.0834 - val_Flatout09_categorical_accuracy: 0.0773 - val_Flatout10_categorical_accuracy: 0.1019 - val_Flatout11_categorical_accuracy: 0.1247 - val_Flatout12_categorical_accuracy: 0.1156 - val_Flatout13_categorical_accuracy: 0.1073 - val_Flatout14_categorical_accuracy: 0.0968 - val_Flatout15_categorical_accuracy: 0.0979 - val_Flatout16_categorical_accuracy: 0.1198 - val_Flatout17_categorical_accuracy: 0.1138 - val_Flatout18_categorical_accuracy: 0.1051 - val_Flatout19_categorical_accuracy: 0.0930 - val_Flatout20_categorical_accuracy: 0.1030 - val_Flatout21_categorical_accuracy: 0.0966 - val_Flatout22_categorical_accuracy: 0.0954 - val_Flatout23_categorical_accuracy: 0.1095 - val_Flatout24_categorical_accuracy: 0.1437 - val_Flatout25_categorical_accuracy: 0.2412 - val_Flatout26_categorical_accuracy: 0.3945 - val_Flatout27_categorical_accuracy: 0.5512 - val_Flatout28_categorical_accuracy: 0.7041 - val_Flatout29_categorical_accuracy: 0.8214 - val_Flatout30_categorical_accuracy: 0.8999 - val_Flatout31_categorical_accuracy: 0.9513 - val_Flatout32_categorical_accuracy: 0.9765 - val_Flatout33_categorical_accuracy: 0.9873 - val_Flatout34_categorical_accuracy: 0.9953 - val_Flatout35_categorical_accuracy: 0.9984 - val_Flatout36_categorical_accuracy: 0.9991 - val_Flatout37_categorical_accuracy: 0.9991 - val_Flatout38_categorical_accuracy: 0.9998 - val_Flatout39_categorical_accuracy: 1.0000\n",
            "Epoch 4/40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UW1CiBhnXnxa",
        "outputId": "037f7b10-6bd3-4c08-989b-f48fad3a61df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -alh | grep model.h5"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root  34M Nov  1 07:44 model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nQux6qLdXabG"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fSDxetlfUEiD",
        "outputId": "1cdf190a-1796-4568-cd28-bdef02247e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  predicts = model.predict(test_text_d, batch_size=32)\n",
        "\n",
        "print(len(predicts))\n",
        "print(predicts[0].shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n",
            "(4000, 41)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xaR0d5VPU23Z",
        "outputId": "1b932c11-b5a5-44ae-f082-a01c0045d94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4627
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10,100):\n",
        "  print(test_text[i])\n",
        "  #syll_mgr.interpret2(predicts[i])\n",
        "  pred = []\n",
        "  for j in range(num_symbols):\n",
        "    pred.append(predicts[j][i])\n",
        "  (x, y) = arpabets_mgr.interpret2(utils.flatten(pred))\n",
        "  print(str(y))\n",
        "  print('')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Of humble seeming: and, behind them all,\n",
            "['AE', 'N', 'D', 'S', 'AH', 'AH', 'AH', 'AH', 'IH', 'N', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'IH', 'N', 'D', 'AH', 'D', 'AH', 'AH', 'S', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Behind his head upon the shoulders lay,\n",
            "['HH', 'IH', 'Z', 'IH', 'N', 'AH', 'IH', 'IH', 'N', 'IH', 'N', 'AH', 'N', 'IH', 'IH', 'IH', 'HH', 'AH', 'IH', 'IH', 'AH', 'AH', 'HH', 'HH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Discover countries, with a kindred heart\n",
            "['AH', 'IH', 'AH', 'N', 'R', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Was open in his praise, and plaudits rose\n",
            "['HH', 'IH', 'Z', 'S', 'R', 'R', 'IH', 'IH', 'AH', 'IH', 'HH', 'IH', 'IH', 'Z', 'AH', 'N', 'IH', 'IH', 'N', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A spear the hero bore of wondrous strength,\n",
            "['AH', 'AH', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'IH', 'T', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Were things indifferent to the Shepherds thoughts.\n",
            "['DH', 'AH', 'T', 'IH', 'D', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', 'N', 'AH', 'AH', 'N', 'D', 'T', 'AH', 'L', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "That to the young supplies a guiding light,\n",
            "['DH', 'AH', 'D', 'IH', 'R', 'AH', 'AH', 'AH', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'DH', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'N', 'AH', 'Z', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of Greeks a mighty army, all in vain;\n",
            "['DH', 'AH', 'D', 'R', 'R', 'AH', 'AH', 'AH', 'N', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Into the mighty vision passing--there,\n",
            "['AH', 'AH', 'AH', 'N', 'DH', 'AH', 'AH', 'AH', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'IH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Them in his anger, whom his anger saves\n",
            "['DH', 'IH', 'R', 'IH', 'R', 'AH', 'AH', 'AH', 'IH', 'IH', 'AH', 'T', 'AH', 'N', 'N', 'IH', 'IH', 'IH', 'Z', 'N', 'Z', 'AH', 'AH', 'IH', 'Z', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And in the soul within the sense began\n",
            "['AE', 'N', 'D', 'W', 'DH', 'DH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'N', 'AH', 'AH', 'IH', 'AH', 'AH', 'D', 'N', 'T', 'D', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "For Daphnis reigns above, and deals from thence\n",
            "['F', 'UW', 'R', 'M', 'R', 'AH', 'AH', 'AH', 'N', 'N', 'N', 'N', 'N', 'S', 'N', 'D', 'AH', 'AH', 'T', 'AH', 'R', 'T', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "For one that looked with such a captains mien:\n",
            "['DH', 'N', 'R', 'R', 'R', 'AH', 'AH', 'AH', 'T', 'T', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of thought between her fingers and her brain,\n",
            "['T', 'ER', 'S', 'IY', 'T', 'AH', 'ER', 'AH', 'R', 'S', 'AH', 'AH', 'AH', 'N', 'AH', 'ER', 'IH', 'N', 'N', 'N', 'N', 'AH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And still the red sedan of rank appeals,\n",
            "['AE', 'N', 'D', 'S', 'AH', 'T', 'AH', 'AH', 'AH', 'T', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "With this eternal silence;--more a god,\n",
            "['W', 'N', 'DH', 'N', 'AH', 'AH', 'AH', 'IH', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'IH', 'N', 'T', 'AH', 'AH', 'IH', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Forgot, nutritious, grateful to the taste,\n",
            "['AE', 'N', 'D', 'P', 'IH', 'V', 'AH', 'AH', 'AH', 'T', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of full perfection prompt his daring dream,\n",
            "['HH', 'IH', 'Z', 'S', 'AH', 'IH', 'IH', 'IH', 'L', 'IH', 'IH', 'AH', 'IH', 'N', 'N', 'N', 'IH', 'IH', 'IH', 'IH', 'D', 'IH', 'IH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And all the spaces of the midnight town\n",
            "['AE', 'N', 'D', 'L', 'R', 'DH', 'DH', 'AH', 'AH', 'IH', 'DH', 'AH', 'AH', 'AH', 'R', 'AH', 'AH', 'AH', 'N', 'AH', 'D', 'T', 'AH', 'N', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The sandbar sings in moonlit veils of foam.\n",
            "['DH', 'AH', 'S', 'R', 'L', 'AH', 'AH', 'AH', 'N', 'S', 'R', 'Z', 'AH', 'AH', 'N', 'N', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'R', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And darkness settled on the outer world.\n",
            "['AE', 'N', 'D', 'N', 'R', 'DH', 'DH', 'AH', 'N', 'IH', 'N', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Was shut and barred, for late has grown the hour.\n",
            "['IH', 'AH', 'S', 'EH', 'T', 'T', 'AH', 'T', 'L', 'T', 'N', 'AH', 'N', 'T', 'AH', 'AH', 'AH', 'AH', 'N', 'T', 'AH', 'AH', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Or castle, high embattled on the point\n",
            "['AO', 'R', 'D', 'IH', 'R', 'DH', 'AH', 'AH', 'N', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The each with All in one, sublime design?\n",
            "['DH', 'AH', 'S', 'M', 'AH', 'AH', 'AH', 'AH', 'L', 'N', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The curving prow, the tall and stately mast,\n",
            "['DH', 'AH', 'S', 'R', 'L', 'AH', 'AH', 'S', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And bids anew the martial thunder rise.\n",
            "['AE', 'N', 'D', 'S', 'R', 'T', 'AH', 'AH', 'IH', 'R', 'IH', 'AH', 'AH', 'AH', 'S', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And hearing harlot muttered twice or thrice,\n",
            "['AE', 'N', 'D', 'S', 'IH', 'N', 'AH', 'IH', 'IH', 'T', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'N', 'N', 'AH', 'AH', 'IH', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "That evening passed with music, chat, and song,\n",
            "['W', 'N', 'D', 'S', 'R', 'T', 'D', 'AH', 'D', 'T', 'D', 'N', 'AH', 'N', 'N', 'N', 'D', 'AH', 'D', 'D', 'D', 'AH', 'IH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "With childlike pleasure at the blooming sun;\n",
            "['W', 'IH', 'DH', 'S', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'N', 'N', 'AH', 'IH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And send her back with gentle words to earth\n",
            "['AE', 'N', 'D', 'S', 'R', 'N', 'HH', 'IH', 'AH', 'S', 'T', 'AH', 'N', 'AH', 'IH', 'AH', 'T', 'N', 'N', 'AH', 'N', 'AH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "With glowing eyes and pale, unsmiling face,\n",
            "['W', 'IH', 'DH', 'S', 'AH', 'N', 'D', 'AH', 'S', 'IH', 'N', 'AH', 'AH', 'N', 'N', 'AH', 'IH', 'N', 'AH', 'AH', 'S', 'AH', 'IH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A threatening tempest fearing, looses wide\n",
            "['AH', 'AH', 'S', 'IH', 'N', 'AH', 'AH', 'S', 'S', 'R', 'AH', 'AH', 'S', 'R', 'N', 'IH', 'AH', 'AH', 'AH', 'T', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Its beauties first? That she will show to me\n",
            "['DH', 'ER', 'R', 'N', 'AH', 'AH', 'N', 'AH', 'T', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'T', 'N', 'N', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A man is likewise counsel for himself,\n",
            "['DH', 'AH', 'S', 'IH', 'EH', 'AH', 'AH', 'IH', 'IH', 'N', 'AH', 'AH', 'N', 'N', 'AH', 'N', 'IH', 'N', 'AH', 'N', 'N', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And awful, mighty eyes that cowed and held\n",
            "['AE', 'N', 'D', 'S', 'EH', 'N', 'D', 'AH', 'S', 'IH', 'L', 'AH', 'AH', 'AH', 'N', 'D', 'DH', 'N', 'AH', 'AH', 'S', 'AH', 'AH', 'AH', 'AH', 'IH', 'R', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of an unusual strength: his mind was keen,\n",
            "['HH', 'IH', 'Z', 'S', 'AH', 'T', 'IH', 'IH', 'Z', 'IH', 'IH', 'AH', 'IH', 'N', 'IH', 'IH', 'IH', 'IH', 'IH', 'D', 'T', 'IH', 'IH', 'AH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of thoughts revolved, his final sentence chose\n",
            "['DH', 'AE', 'T', 'W', 'EH', 'T', 'IH', 'IH', 'IH', 'T', 'AH', 'IH', 'T', 'IH', 'AH', 'N', 'IH', 'IH', 'N', 'D', 'D', 'IH', 'IH', 'IH', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "His head, his shoulders, and his knees embraced;\n",
            "['HH', 'IH', 'Z', 'S', 'EH', 'HH', 'IH', 'IH', 'Z', 'IH', 'IH', 'IH', 'IH', 'N', 'IH', 'IH', 'HH', 'IH', 'IH', 'IH', 'IH', 'IH', 'HH', 'IH', 'IH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And came into the vale in little space,\n",
            "['AE', 'N', 'D', 'S', 'R', 'DH', 'D', 'AH', 'AH', 'IH', 'D', 'AH', 'AH', 'AH', 'R', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'N', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "In forest wild, in thicket, brake, or den;\n",
            "['AH', 'R', 'L', 'R', 'N', 'AH', 'AH', 'R', 'R', 'N', 'N', 'N', 'R', 'R', 'AH', 'D', 'AH', 'AH', 'R', 'R', 'AH', 'T', 'T', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "In spite of changes. Look for me until\n",
            "['F', 'AH', 'R', 'N', 'T', 'T', 'AH', 'AH', 'R', 'AH', 'N', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AE', 'N', 'N', 'D', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The parchment of the future, it were vain--\n",
            "['DH', 'AH', 'S', 'R', 'R', 'AH', 'AH', 'AH', 'AH', 'T', 'IH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'IH', 'N', 'T', 'AH', 'AH', 'L', 'T', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And there, myself and two beloved Friends,\n",
            "['AE', 'N', 'D', 'L', 'AH', 'N', 'AH', 'AH', 'T', 'AH', 'AH', 'N', 'AH', 'AH', 'D', 'AH', 'N', 'AE', 'AE', 'AH', 'D', 'R', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The spirits of the Seasons seem to stand.\n",
            "['DH', 'AH', 'S', 'N', 'DH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'D', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "For lo! his passion, but an art of craft,\n",
            "['F', 'IH', 'R', 'M', 'T', 'AH', 'AH', 'T', 'N', 'R', 'AH', 'AH', 'IH', 'N', 'AH', 'AH', 'IH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Is due, and not to war, intrepid still\n",
            "['IH', 'Z', 'N', 'AO', 'R', 'T', 'AH', 'AH', 'IH', 'T', 'AH', 'AH', 'R', 'T', 'AH', 'T', 'AH', 'AH', 'R', 'T', 'T', 'T', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Or sleeping, walk a restless world of dreams.\n",
            "['AO', 'N', 'D', 'L', 'R', 'DH', 'AH', 'AH', 'IH', 'IH', 'N', 'AH', 'AH', 'AH', 'R', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of teaching school, a school of Czechs and Poles\n",
            "['AH', 'V', 'S', 'EH', 'R', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'R', 'AH', 'AH', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And drops of blood bedew the conscious lawn;\n",
            "['AE', 'N', 'D', 'D', 'L', 'N', 'AH', 'AH', 'N', 'S', 'R', 'Z', 'AH', 'AH', 'AH', 'N', 'DH', 'AH', 'AH', 'AH', 'S', 'N', 'AH', 'N', 'AH', 'R', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "From daylight till the evening, wet or fine,\n",
            "['IH', 'V', 'L', 'IH', 'N', 'DH', 'AH', 'T', 'R', 'AH', 'N', 'AH', 'R', 'IH', 'AH', 'AH', 'N', 'AH', 'N', 'D', 'AH', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A doom that ever poised itself to fall,\n",
            "['DH', 'AH', 'T', 'N', 'AH', 'AH', 'AH', 'AH', 'T', 'IH', 'IH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of wailing women pierce the vaulted skies.\n",
            "['DH', 'N', 'D', 'EH', 'R', 'AH', 'AH', 'AH', 'S', 'R', 'R', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'S', 'AH', 'N', 'AH', 'AH', 'AH', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "To happy havens under all the sky,\n",
            "['F', 'N', 'R', 'M', 'DH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'L', 'AH', 'D', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "In purple clusters hangs the vines increase,\n",
            "['DH', 'AH', 'S', 'R', 'N', 'AH', 'AH', 'Z', 'AH', 'L', 'L', 'AH', 'AH', 'N', 'AH', 'N', 'DH', 'AH', 'AH', 'AH', 'AH', 'AH', 'R', 'AH', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A shepherd meeting thee, the oar surveys,\n",
            "['DH', 'AH', 'S', 'R', 'S', 'AH', 'AH', 'AH', 'L', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The printed passion of the dream remains:--\n",
            "['DH', 'AH', 'S', 'N', 'R', 'IH', 'AH', 'AH', 'L', 'T', 'R', 'AH', 'AH', 'N', 'AH', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'N', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "His aged blood exhausted sees, and pours\n",
            "['HH', 'IH', 'Z', 'S', 'EH', 'Z', 'IH', 'Z', 'IH', 'R', 'R', 'IH', 'IH', 'Z', 'HH', 'IH', 'IH', 'IH', 'IH', 'Z', 'Z', 'IH', 'L', 'IH', 'IH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And piecemeal shares and maims the felon round.\n",
            "['AE', 'N', 'D', 'S', 'R', 'N', 'AH', 'AH', 'AH', 'T', 'R', 'AH', 'AH', 'T', 'AH', 'AH', 'N', 'AH', 'T', 'AH', 'AH', 'T', 'AH', 'N', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "A happy nation, and a happy king.\n",
            "['DH', 'AH', 'HH', 'N', 'R', 'AH', 'AH', 'T', 'L', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And Miscreator, makes and helps along\n",
            "['AE', 'N', 'D', 'S', 'IH', 'T', 'IH', 'AH', 'M', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'IH', 'T', 'N', 'AH', 'N', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The dashing waters when the air is still\n",
            "['DH', 'AH', 'S', 'IH', 'N', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And still its ice the freezing silence kept:\n",
            "['AE', 'N', 'D', 'S', 'N', 'T', 'T', 'AH', 'IH', 'IH', 'IH', 'T', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'S', 'T', 'AH', 'T', 'N', 'IH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "That shrink in misty mournfulness from sight,\n",
            "['DH', 'N', 'T', 'IH', 'R', 'IH', 'AH', 'AH', 'AH', 'R', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'DH', 'AH', 'AH', 'AH', 'AH', 'T', 'N', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And secret seeds of envy, lay behind;\n",
            "['AE', 'N', 'D', 'L', 'R', 'DH', 'AH', 'AH', 'AH', 'IH', 'R', 'AH', 'AH', 'AH', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of trees and crack of branches, common things,\n",
            "['AH', 'V', 'D', 'IH', 'L', 'AH', 'D', 'AH', 'N', 'N', 'D', 'N', 'AH', 'N', 'AH', 'D', 'N', 'AH', 'AE', 'D', 'N', 'N', 'D', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And pious tribute at her altars pay:\n",
            "['AE', 'N', 'D', 'B', 'R', 'N', 'AH', 'AH', 'AH', 'S', 'R', 'AH', 'AH', 'AH', 'R', 'AH', 'T', 'AH', 'D', 'AH', 'AH', 'AH', 'AH', 'L', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Courageous, and refreshed for future toil,\n",
            "['F', 'AO', 'R', 'T', 'R', 'AH', 'AH', 'T', 'T', 'T', 'AH', 'IH', 'T', 'AH', 'N', 'R', 'AH', 'IH', 'N', 'T', 'T', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Upon the wall and windows blots of gilt.\n",
            "['AH', 'V', 'R', 'IH', 'DH', 'AH', 'AH', 'AH', 'N', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'D', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And lay upon the threshold like a log.\n",
            "['AE', 'N', 'D', 'W', 'IH', 'DH', 'AH', 'IH', 'AH', 'N', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'T', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "In synod unbenign; and taught the fixed\n",
            "['DH', 'N', 'D', 'R', 'R', 'AH', 'AH', 'IH', 'N', 'N', 'D', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And left him to his rest.  An end of Love.\n",
            "['AE', 'N', 'D', 'S', 'IY', 'HH', 'HH', 'IH', 'IH', 'IH', 'HH', 'HH', 'IH', 'Z', 'IH', 'N', 'HH', 'IH', 'N', 'D', 'HH', 'HH', 'HH', 'IH', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And human hearts and minds to show it by,\n",
            "['AE', 'N', 'D', 'S', 'EH', 'N', 'AH', 'AH', 'AH', 'R', 'R', 'AH', 'AH', 'AH', 'N', 'R', 'T', 'N', 'N', 'AH', 'N', 'R', 'AH', 'AH', 'AH', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The outward struggle and the inward strife.\n",
            "['DH', 'AH', 'S', 'R', 'T', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'R', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "His arms and bosom, seem to melt away.\n",
            "['HH', 'IH', 'Z', 'S', 'L', 'HH', 'IH', 'Z', 'Z', 'R', 'R', 'HH', 'IH', 'Z', 'HH', 'IH', 'HH', 'HH', 'IH', 'Z', 'Z', 'HH', 'HH', 'IH', 'IH', 'R', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And noise, and tumult rises from the crowd.\n",
            "['AE', 'N', 'D', 'S', 'AW', 'N', 'AH', 'AH', 'S', 'N', 'D', 'Z', 'AH', 'AH', 'EH', 'N', 'DH', 'IH', 'AH', 'IH', 'IH', 'N', 'AH', 'AH', 'AH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Cannot devise a torment, so it be\n",
            "['IH', 'AO', 'S', 'N', 'N', 'T', 'AH', 'IH', 'T', 'AH', 'N', 'IH', 'AH', 'N', 'N', 'IH', 'IH', 'T', 'S', 'D', 'AH', 'D', 'IH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The damsel put the pair without reply:\n",
            "['DH', 'AH', 'S', 'IH', 'N', 'T', 'N', 'AH', 'S', 'R', 'T', 'N', 'D', 'N', 'T', 'IH', 'D', 'AH', 'N', 'T', 'T', 'AH', 'L', 'AH', 'D', 'R', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Or that it hardens more and helps to bind\n",
            "['AO', 'R', 'D', 'T', 'IH', 'IH', 'AH', 'IH', 'T', 'S', 'IH', 'AH', 'AH', 'T', 'AH', 'N', 'AH', 'AH', 'S', 'T', 'AH', 'AH', 'N', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And flowing waters, and the starry flame,\n",
            "['AE', 'N', 'D', 'S', 'R', 'N', 'AH', 'AH', 'AH', 'IH', 'R', 'Z', 'AH', 'AH', 'R', 'S', 'DH', 'AH', 'AH', 'AH', 'D', 'AH', 'AH', 'AH', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The spoil or spoiler? this behold in me;\n",
            "['DH', 'AH', 'T', 'N', 'DH', 'AH', 'AH', 'AH', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And rode with them, and spake to none; the day\n",
            "['AE', 'N', 'D', 'W', 'EY', 'DH', 'D', 'AH', 'W', 'N', 'D', 'AH', 'D', 'AH', 'R', 'D', 'AH', 'AH', 'N', 'D', 'D', 'R', 'AH', 'N', 'D', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of finest metal was her armour bright,\n",
            "['AH', 'ER', 'R', 'IH', 'R', 'AH', 'ER', 'AH', 'AH', 'R', 'R', 'AH', 'AH', 'AH', 'N', 'AH', 'T', 'AH', 'N', 'S', 'S', 'AH', 'AH', 'AH', 'L', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "The doleful city all around invest,\n",
            "['DH', 'AH', 'S', 'R', 'DH', 'AH', 'AH', 'AH', 'R', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Of crowded cities, was from passing tale\n",
            "['DH', 'AH', 'S', 'IH', 'DH', 'AH', 'AH', 'AH', 'N', 'D', 'D', 'AH', 'AH', 'N', 'AH', 'D', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', 'S', 'N', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "An Arab fearless as the face of Truth\n",
            "['AH', 'AH', 'S', 'N', 'T', 'AH', 'AH', 'T', 'L', 'T', 'AH', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'S', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "It lightened toil, and took the edge from pain,\n",
            "['DH', 'AH', 'S', 'S', 'R', 'T', 'T', 'IH', 'AH', 'T', 'N', 'IH', 'AH', 'N', 'N', 'D', 'T', 'IH', 'N', 'D', 'AH', 'T', 'N', 'T', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "That brought with it another civil war.\n",
            "['DH', 'N', 'T', 'S', 'R', 'AH', 'D', 'AH', 'AH', 'IH', 'D', 'AH', 'AH', 'T', 'AH', 'N', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'AH', 'T', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Apart from all, in pride of conscious strength,\n",
            "['W', 'N', 'DH', 'S', 'R', 'T', 'IH', 'IH', 'T', 'R', 'N', 'AH', 'AH', 'AH', 'N', 'R', 'IH', 'N', 'T', 'AH', 'N', 'AH', 'N', 'AH', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "And far and near throughout the land the men\n",
            "['AE', 'N', 'D', 'M', 'R', 'DH', 'DH', 'AH', 'N', 'N', 'DH', 'AH', 'AH', 'AH', 'N', 'D', 'AH', 'AH', 'AH', 'AH', 'N', 'AH', 'AH', 'N', 'AH', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "Remembered--I. And, oh! a little while\n",
            "['AH', 'IH', 'AH', 'N', 'T', 'AH', 'AH', 'AH', 'T', 'AH', 'AH', 'IH', 'AH', 'N', 'AH', 'AH', 'N', 'D', 'AH', 'N', 'AH', 'T', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yyDGVtigW57f",
        "outputId": "cd5e1dac-56bd-4d76-95be-719d4324fa46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "categories = df_train.label.cat.categories.tolist()\n",
        "predict_logits = predicts.argmax(axis=1)\n",
        "print(\"Categorie: {0}\".format(categories))\n",
        "predict_labels = [categories[logit] for logit in predict_logits]\n",
        "predict_labels"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7f6e6bf5580f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Categorie: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict_logits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hYhmukbSKpnp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "os.remove('./model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}