{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Meter: Keras + Universal Sentence Encoder + CMUdict + Project Gutenberg poetry archives",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "co7MV6sX7Xto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [Keras + Universal Sentence Encoder = Deep Meter](https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/) "
      ]
    },
    {
      "metadata": {
        "id": "eAVQGidpL8v5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook creates an autoencoder using the Universal Sentence Encoder. The autoencoder output is CMUdict syllables. The dataset is that subset of Allison Parrish's Project Gutenberg poetry archive which happens to scan in iambic pentameter.\n",
        "\n",
        "The notebook is based on Chengwei Zhang's example of wrapping the USE inside a larger tensorflow model saves to a Keras model (without save the USE itself in the TF model).\n",
        "\n",
        "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n",
        "\n",
        "Since there are 10 one-hot values for 10 sets of 6k syllables, this is \"multi-label classification\"\n",
        "Changes for multi-label classification:\n",
        "sigmoid activation instead of softmax\n",
        "binary_crossentropy\n",
        "\n",
        "Text format is tab-separated, 2 columns: first text, second multi-level\n",
        "array of syllables:\n"
      ]
    },
    {
      "metadata": {
        "id": "pOTzp8O36CyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs."
      ]
    },
    {
      "metadata": {
        "id": "lVjNK8shFKOC",
        "colab_type": "code",
        "outputId": "94429948-36b5-4ccb-85ce-5d11ce578334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "%cd /content\n",
        "!git clone https://github.com/LanceNorskog/deep_meter || true\n",
        "%cd deep_meter\n",
        "!git pull\n",
        "# could not figure out how to read gzipped files as text!\n",
        "!gunzip -qf blobs/*.gz || true\n",
        "!gunzip -qf prepped_data/*.gz || true"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deep_meter'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 225 (delta 121), reused 161 (delta 60), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (225/225), 20.72 MiB | 8.55 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "/content/deep_meter\n",
            "Already up to date.\n",
            "gzip: blobs/*.gz: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSeY-MUQo2Ha",
        "colab_type": "code",
        "outputId": "900b7c75-0aa9-4f3f-ad0c-d562cc50e7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# boilerplate from base notebook\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "np.random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "feBc_8Y-pt6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# github deep_meter code\n",
        "import utils\n",
        "# should not need this to use utils.flatten but is true anyway?\n",
        "from itertools import chain\n",
        "import subprocess\n",
        "import syllables\n",
        "# misc for this notebook\n",
        "from ast import literal_eval\n",
        "\n",
        "import scipy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwty8Z6mAkdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8F4LNGFqOiq",
        "colab_type": "code",
        "outputId": "4426a0d8-1343-440b-a995-c36760050467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FwAQNy1eMDkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read classified poetry lines: text tab [['syll', 'la', 'ble'], ...]\n",
        "# clip to only most common syllables with syllable manager\n",
        "def get_data(filename, syll_mgr, num_symbols):\n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    text_lines = []\n",
        "    enc_array = []\n",
        "    num_syllables = syll_mgr.get_size()\n",
        "    for i in range(0, len(lines)):\n",
        "      parts = lines[i].split(\"\\t\")\n",
        "      label = utils.flatten(literal_eval(parts[1]))\n",
        "      if len(label) != num_symbols:\n",
        "        continue\n",
        "      enc = np.zeros((num_symbols * syll_mgr.get_size()), dtype=np.int8)\n",
        "      for j in range(num_symbols):\n",
        "        enc[num_syllables * j + syll_mgr.get_encoding(label[j])] = 1\n",
        "      text_lines.append([parts[0]])\n",
        "      enc_array.append(enc)\n",
        "\n",
        "    return (np.array(text_lines), np.array(enc_array))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3HAtd4X5DayF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# syllables in descending order of occurrence - 6k in gutenberg.iambic_pentameter, 15k total\n",
        "# clamp to most common 100 syllables while debugging- use NCE to get all syllables or interesting number\n",
        "# 98 + pause + wildcard\n",
        "num_syllables = 1800 \n",
        "# iambic pentameter\n",
        "num_symbols = 10\n",
        "syll_mgr = syllables.syllables(num_syllables)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eN9aqig-QpDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(train_text, train_label) = get_data('prepped_data/gutenberg.iambic_pentameter.train', syll_mgr, num_symbols)\n",
        "num_training = len(train_text)\n",
        "#train_text = train_text[0:10000]\n",
        "#train_label = train_label[0:10000]\n",
        "\n",
        "(test_text, test_label) = get_data('prepped_data/gutenberg.iambic_pentameter.test', syll_mgr, num_symbols)\n",
        "num_testing = len(test_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mQUV_7dIm-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# slow\n",
        "num_epochs = 100\n",
        "adam_0001 = tf.train.AdamOptimizer(0.00003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sf9A4Xl6J7c6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrap embed module in a Lambda layer\n",
        "Explicitly cast the input as a string"
      ]
    },
    {
      "metadata": {
        "id": "PRD3fWgJjOrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qX2rBOuxDP1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assemble model"
      ]
    },
    {
      "metadata": {
        "id": "t3fllZkVjXKV",
        "colab_type": "code",
        "outputId": "41d97dfe-0e9a-4fea-a783-5ebd173b9dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# changed accuracy from 'choose your own accuracy'\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "dense = layers.Dense(768, activation='relu', name='Convoluted')(embedding)\n",
        "dense = layers.Dense(1024, activation='relu', name='Medium')(dense)\n",
        "dense = layers.Dense(2048, activation='relu', name='Smooth')(dense)\n",
        "pred = layers.Dense(num_syllables * num_symbols, activation='sigmoid')(dense)\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam_0001, metrics=['binary_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "Convoluted (Dense)           (None, 768)               393984    \n",
            "_________________________________________________________________\n",
            "Medium (Dense)               (None, 1024)              787456    \n",
            "_________________________________________________________________\n",
            "Smooth (Dense)               (None, 2048)              2099200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 18000)             36882000  \n",
            "=================================================================\n",
            "Total params: 40,162,640\n",
            "Trainable params: 40,162,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9PfsPdG8FZBI",
        "colab_type": "code",
        "outputId": "9eae0daa-e573-444f-cda3-bca2fffcc92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_label.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4200, 18000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "gPYVmBr2Fbob",
        "colab_type": "code",
        "outputId": "f8bb3c54-329b-4eaf-c479-b6d40df86f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_text[200])\n",
        "syll_mgr.interpret2(test_label[200])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In this: whatever other blunders lie']\n",
            "[67, 224, 213, 65, 47, 36, 280, 0, 487, 901]\n",
            "['IH N', 'DH IH S', 'W AH T', 'EH', 'V ER', 'AH', 'DH ER', '?', 'D ER Z', 'L AY']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqcRy_JWXe0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Keras model and save weights\n",
        "This only trains and save our Keras layers not the embed module' weights."
      ]
    },
    {
      "metadata": {
        "id": "_stfC_7VFhS8",
        "colab_type": "code",
        "outputId": "4964b64d-033f-417a-802e-6f1a3c9e0542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1131
        }
      },
      "cell_type": "code",
      "source": [
        "use_saved_model=True\n",
        "retrain=False\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    if retrain:\n",
        "      model.load_weights('./model.h5')  \n",
        "    else:\n",
        "      session.run(tf.tables_initializer())\n",
        "    history = model.fit(train_text, \n",
        "            train_label,\n",
        "            validation_data=(test_text, test_label),\n",
        "            epochs=num_epochs,\n",
        "            callbacks = [EarlyStopping(patience=2)],\n",
        "            batch_size=32)\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 58498 samples, validate on 4200 samples\n",
            "Epoch 1/100\n",
            "58498/58498 [==============================] - 122s 2ms/step - loss: 0.0318 - binary_accuracy: 0.9960 - val_loss: 0.0034 - val_binary_accuracy: 0.9994\n",
            "Epoch 2/100\n",
            "58498/58498 [==============================] - 116s 2ms/step - loss: 0.0033 - binary_accuracy: 0.9994 - val_loss: 0.0033 - val_binary_accuracy: 0.9994\n",
            "Epoch 3/100\n",
            "58498/58498 [==============================] - 116s 2ms/step - loss: 0.0033 - binary_accuracy: 0.9994 - val_loss: 0.0033 - val_binary_accuracy: 0.9994\n",
            "Epoch 4/100\n",
            "58498/58498 [==============================] - 115s 2ms/step - loss: 0.0033 - binary_accuracy: 0.9994 - val_loss: 0.0033 - val_binary_accuracy: 0.9995\n",
            "Epoch 5/100\n",
            "58498/58498 [==============================] - 115s 2ms/step - loss: 0.0033 - binary_accuracy: 0.9995 - val_loss: 0.0033 - val_binary_accuracy: 0.9995\n",
            "Epoch 6/100\n",
            "58498/58498 [==============================] - 119s 2ms/step - loss: 0.0032 - binary_accuracy: 0.9995 - val_loss: 0.0032 - val_binary_accuracy: 0.9995\n",
            "Epoch 7/100\n",
            "58498/58498 [==============================] - 119s 2ms/step - loss: 0.0032 - binary_accuracy: 0.9995 - val_loss: 0.0032 - val_binary_accuracy: 0.9995\n",
            "Epoch 8/100\n",
            "58498/58498 [==============================] - 119s 2ms/step - loss: 0.0031 - binary_accuracy: 0.9995 - val_loss: 0.0031 - val_binary_accuracy: 0.9995\n",
            "Epoch 9/100\n",
            "58498/58498 [==============================] - 121s 2ms/step - loss: 0.0031 - binary_accuracy: 0.9995 - val_loss: 0.0031 - val_binary_accuracy: 0.9995\n",
            "Epoch 10/100\n",
            "58498/58498 [==============================] - 126s 2ms/step - loss: 0.0030 - binary_accuracy: 0.9995 - val_loss: 0.0030 - val_binary_accuracy: 0.9995\n",
            "Epoch 11/100\n",
            "58498/58498 [==============================] - 124s 2ms/step - loss: 0.0030 - binary_accuracy: 0.9995 - val_loss: 0.0030 - val_binary_accuracy: 0.9995\n",
            "Epoch 12/100\n",
            "58498/58498 [==============================] - 126s 2ms/step - loss: 0.0029 - binary_accuracy: 0.9995 - val_loss: 0.0030 - val_binary_accuracy: 0.9995\n",
            "Epoch 13/100\n",
            "58498/58498 [==============================] - 130s 2ms/step - loss: 0.0029 - binary_accuracy: 0.9995 - val_loss: 0.0029 - val_binary_accuracy: 0.9995\n",
            "Epoch 14/100\n",
            "58498/58498 [==============================] - 130s 2ms/step - loss: 0.0028 - binary_accuracy: 0.9995 - val_loss: 0.0029 - val_binary_accuracy: 0.9995\n",
            "Epoch 15/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0028 - binary_accuracy: 0.9995 - val_loss: 0.0029 - val_binary_accuracy: 0.9995\n",
            "Epoch 16/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0028 - binary_accuracy: 0.9995 - val_loss: 0.0029 - val_binary_accuracy: 0.9995\n",
            "Epoch 17/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0027 - binary_accuracy: 0.9995 - val_loss: 0.0029 - val_binary_accuracy: 0.9995\n",
            "Epoch 18/100\n",
            "58498/58498 [==============================] - 132s 2ms/step - loss: 0.0027 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 19/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0027 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 20/100\n",
            "58498/58498 [==============================] - 133s 2ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 21/100\n",
            "58498/58498 [==============================] - 132s 2ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 22/100\n",
            "58498/58498 [==============================] - 132s 2ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 23/100\n",
            "58498/58498 [==============================] - 134s 2ms/step - loss: 0.0026 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 24/100\n",
            "58498/58498 [==============================] - 133s 2ms/step - loss: 0.0025 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 25/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0025 - binary_accuracy: 0.9995 - val_loss: 0.0028 - val_binary_accuracy: 0.9995\n",
            "Epoch 26/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0025 - binary_accuracy: 0.9995 - val_loss: 0.0027 - val_binary_accuracy: 0.9995\n",
            "Epoch 27/100\n",
            "58498/58498 [==============================] - 128s 2ms/step - loss: 0.0024 - binary_accuracy: 0.9995 - val_loss: 0.0027 - val_binary_accuracy: 0.9995\n",
            "Epoch 28/100\n",
            "58498/58498 [==============================] - 130s 2ms/step - loss: 0.0024 - binary_accuracy: 0.9995 - val_loss: 0.0027 - val_binary_accuracy: 0.9995\n",
            "Epoch 29/100\n",
            "58498/58498 [==============================] - 132s 2ms/step - loss: 0.0024 - binary_accuracy: 0.9995 - val_loss: 0.0027 - val_binary_accuracy: 0.9995\n",
            "Epoch 30/100\n",
            "58498/58498 [==============================] - 131s 2ms/step - loss: 0.0023 - binary_accuracy: 0.9995 - val_loss: 0.0027 - val_binary_accuracy: 0.9995\n",
            "Epoch 31/100\n",
            "15616/58498 [=======>......................] - ETA: 1:31 - loss: 0.0023 - binary_accuracy: 0.9995Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UW1CiBhnXnxa",
        "colab_type": "code",
        "outputId": "0d9a3ffc-b320-48f2-eadc-dda8d646b555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -alh | grep model.h5"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 154M Oct 26 19:37 model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nQux6qLdXabG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ]
    },
    {
      "metadata": {
        "id": "fSDxetlfUEiD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  predicts = model.predict(test_text, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9V5JOWGq4Cd",
        "colab_type": "code",
        "outputId": "4680789c-47f2-4d88-affc-31bc8b93c1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "first = predicts[0]\n",
        "max_d = -1000000\n",
        "min_d = 1000000\n",
        "max_i = 0\n",
        "min_i = 0\n",
        "for i in range(len(predicts) - 1):\n",
        "  v = predicts[i]\n",
        "  d = scipy.spatial.distance.cosine(v, first)\n",
        "  if max_d < d:\n",
        "    max_d = d\n",
        "    max_i = i\n",
        "  if min_d > d:\n",
        "    min_d = d\n",
        "    min_i = i\n",
        "print(\"Min and max distances: {0} and {1}\".format(min_d, max_d))\n",
        "print(test_text[max_i])\n",
        "syll_mgr.interpret2(predicts[max_i])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min and max distances: 2.4950421217972973e-07 and 0.924818860938364\n",
            "['His fathers blessing from an elder son:']\n",
            "[10, 571, 280, 571, 280, 40, 10, 571, 972, 180]\n",
            "['HH IH Z', 'F AA', 'DH ER', 'F AA', 'DH ER', 'AH V', 'HH IH Z', 'F AA', 'DH ER Z', 'S AH N']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xaR0d5VPU23Z",
        "colab_type": "code",
        "outputId": "ad68a7c4-5e0c-47bf-f78d-ef0095304b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6269
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(10,100):\n",
        "  print(test_text[i])\n",
        "  syll_mgr.interpret2(predicts[i])\n",
        "  print('')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Of humble seeming: and, behind them all,']\n",
            "[40, 1514, 7, 0, 7, 1514, 7, 0, 153, 0]\n",
            "['AH V', 'HH AH M', 'AE N D', '?', 'AE N D', 'HH AH M', 'AE N D', '?', 'DH EH M', '?']\n",
            "\n",
            "['Behind his head upon the shoulders lay,']\n",
            "[36, 936, 10, 1436, 7, 445, 10, 0, 10, 445]\n",
            "['AH', 'N IY TH', 'HH IH Z', 'N EH K', 'AE N D', 'HH EH D', 'HH IH Z', '?', 'HH IH Z', 'HH EH D']\n",
            "\n",
            "['Discover countries, with a kindred heart']\n",
            "[30, 0, 31, 0, 0, 7, 36, 0, 117, 417]\n",
            "['T UW', '?', 'DH AH', '?', '?', 'AE N D', 'AH', '?', 'SH AH N Z', 'W ER L D']\n",
            "\n",
            "['Was open in his praise, and plaudits rose']\n",
            "[156, 1219, 23, 0, 7, 7, 10, 0, 10, 0]\n",
            "['W AA Z', 'S IY V D', 'T AH D', '?', 'AE N D', 'AE N D', 'HH IH Z', '?', 'HH IH Z', '?']\n",
            "\n",
            "['A spear the hero bore of wondrous strength,']\n",
            "[36, 166, 40, 685, 40, 91, 36, 0, 18, 0]\n",
            "['AH', 'M AY', 'AH V', 'S AO R D', 'AH V', 'S P IH R', 'AH', '?', 'T AH L', '?']\n",
            "\n",
            "['Were things indifferent to the Shepherds thoughts.']\n",
            "[156, 0, 31, 0, 766, 0, 31, 0, 31, 24]\n",
            "['W AA Z', '?', 'DH AH', '?', 'F R AH N T', '?', 'DH AH', '?', 'DH AH', 'TH AO T S']\n",
            "\n",
            "['That to the young supplies a guiding light,']\n",
            "[62, 0, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['F AO R', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['Of Greeks a mighty army, all in vain;']\n",
            "[40, 190, 40, 0, 0, 0, 40, 0, 40, 0]\n",
            "['AH V', 'TH AW', 'AH V', '?', '?', '?', 'AH V', '?', 'AH V', '?']\n",
            "\n",
            "['Into the mighty vision passing--there,']\n",
            "[36, 30, 31, 166, 31, 53, 31, 0, 31, 0]\n",
            "['AH', 'T UW', 'DH AH', 'M AY', 'DH AH', 'G L AO', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['Them in his anger, whom his anger saves']\n",
            "[62, 213, 10, 0, 10, 716, 10, 245, 10, 0]\n",
            "['F AO R', 'W AH T', 'HH IH Z', '?', 'HH IH Z', 'DH EY', 'HH IH Z', 'AE NG', 'HH IH Z', '?']\n",
            "\n",
            "['And in the soul within the sense began']\n",
            "[7, 67, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['AE N D', 'IH N', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['For Daphnis reigns above, and deals from thence']\n",
            "[30, 0, 7, 0, 30, 0, 7, 584, 7, 0]\n",
            "['T UW', '?', 'AE N D', '?', 'T UW', '?', 'AE N D', 'DH AY', 'AE N D', '?']\n",
            "\n",
            "['For one that looked with such a captains mien:']\n",
            "[62, 708, 36, 0, 0, 0, 36, 0, 36, 0]\n",
            "['F AO R', 'S AH M', 'AH', '?', '?', '?', 'AH', '?', 'AH', '?']\n",
            "\n",
            "['Of thought between her fingers and her brain,']\n",
            "[36, 359, 170, 0, 170, 710, 170, 425, 170, 818]\n",
            "['AH', 'B AW T', 'HH ER', '?', 'HH ER', 'AY Z', 'HH ER', 'HH Y UW', 'HH ER', 'B R EY N']\n",
            "\n",
            "['And still the red sedan of rank appeals,']\n",
            "[7, 133, 31, 0, 31, 0, 31, 0, 0, 0]\n",
            "['AE N D', 'AO L', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['With this eternal silence;--more a god,']\n",
            "[76, 278, 36, 6, 16, 439, 16, 17, 36, 439]\n",
            "['W IH DH', 'S AH CH', 'AH', 'T ER', 'IH', 'G AA D', 'IH', 'M AO R', 'AH', 'G AA D']\n",
            "\n",
            "['Forgot, nutritious, grateful to the taste,']\n",
            "[262, 0, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['F ER', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['Of full perfection prompt his daring dream,']\n",
            "[10, 0, 403, 0, 10, 0, 10, 0, 10, 0]\n",
            "['HH IH Z', '?', 'T IY', '?', 'HH IH Z', '?', 'HH IH Z', '?', 'HH IH Z', '?']\n",
            "\n",
            "['And all the spaces of the midnight town']\n",
            "[7, 133, 31, 0, 31, 40, 31, 40, 31, 0]\n",
            "['AE N D', 'AO L', 'DH AH', '?', 'DH AH', 'AH V', 'DH AH', 'AH V', 'DH AH', '?']\n",
            "\n",
            "['The sandbar sings in moonlit veils of foam.']\n",
            "[31, 0, 0, 0, 0, 0, 31, 0, 31, 0]\n",
            "['DH AH', '?', '?', '?', '?', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['And darkness settled on the outer world.']\n",
            "[7, 307, 31, 307, 31, 307, 31, 417, 31, 509]\n",
            "['AE N D', 'D AA R K', 'DH AH', 'D AA R K', 'DH AH', 'D AA R K', 'DH AH', 'W ER L D', 'DH AH', 'ER TH']\n",
            "\n",
            "['Or castle, high embattled on the point']\n",
            "[50, 0, 31, 0, 0, 0, 31, 0, 0, 0]\n",
            "['AO R', '?', 'DH AH', '?', '?', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['The each with All in one, sublime design?']\n",
            "[31, 1286, 40, 0, 7, 133, 31, 133, 7, 832]\n",
            "['DH AH', 'N AH M', 'AH V', '?', 'AE N D', 'AO L', 'DH AH', 'AO L', 'AE N D', 'DH IY Z']\n",
            "\n",
            "['The curving prow, the tall and stately mast,']\n",
            "[31, 0, 31, 0, 31, 0, 31, 0, 0, 0]\n",
            "['DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['And bids anew the martial thunder rise.']\n",
            "[7, 0, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['AE N D', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['And hearing harlot muttered twice or thrice,']\n",
            "[7, 0, 0, 0, 36, 0, 36, 0, 36, 0]\n",
            "['AE N D', '?', '?', '?', 'AH', '?', 'AH', '?', 'AH', '?']\n",
            "\n",
            "['That evening passed with music, chat, and song,']\n",
            "[36, 539, 156, 0, 7, 983, 31, 0, 7, 0]\n",
            "['AH', 'IY V', 'W AA Z', '?', 'AE N D', 'M Y UW', 'DH AH', '?', 'AE N D', '?']\n",
            "\n",
            "['With childlike pleasure at the blooming sun;']\n",
            "[76, 0, 31, 0, 0, 40, 36, 40, 31, 0]\n",
            "['W IH DH', '?', 'DH AH', '?', '?', 'AH V', 'AH', 'AH V', 'DH AH', '?']\n",
            "\n",
            "['And send her back with gentle words to earth']\n",
            "[7, 0, 170, 170, 170, 170, 170, 170, 170, 0]\n",
            "['AE N D', '?', 'HH ER', 'HH ER', 'HH ER', 'HH ER', 'HH ER', 'HH ER', 'HH ER', '?']\n",
            "\n",
            "['With glowing eyes and pale, unsmiling face,']\n",
            "[76, 710, 0, 710, 7, 307, 36, 1261, 1320, 710]\n",
            "['W IH DH', 'AY Z', '?', 'AY Z', 'AE N D', 'D AA R K', 'AH', 'G L OW', 'K AH S T', 'AY Z']\n",
            "\n",
            "['Its beauties first? That she will show to me']\n",
            "[224, 400, 403, 400, 62, 454, 170, 454, 89, 59]\n",
            "['DH IH S', 'AY', 'T IY', 'AY', 'F AO R', 'L AH V', 'HH ER', 'L AH V', 'B IH', 'M IY']\n",
            "\n",
            "['A man is likewise counsel for himself,']\n",
            "[67, 511, 36, 0, 36, 0, 86, 0, 36, 0]\n",
            "['IH N', 'M AE N', 'AH', '?', 'AH', '?', 'IH Z', '?', 'AH', '?']\n",
            "\n",
            "['And awful, mighty eyes that cowed and held']\n",
            "[7, 133, 76, 0, 7, 710, 7, 0, 7, 710]\n",
            "['AE N D', 'AO L', 'W IH DH', '?', 'AE N D', 'AY Z', 'AE N D', '?', 'AE N D', 'AY Z']\n",
            "\n",
            "['Of thoughts revolved, his final sentence chose']\n",
            "[10, 0, 10, 0, 10, 0, 10, 213, 195, 0]\n",
            "['HH IH Z', '?', 'HH IH Z', '?', 'HH IH Z', '?', 'HH IH Z', 'W AH T', 'R IH', '?']\n",
            "\n",
            "['His head, his shoulders, and his knees embraced;']\n",
            "[10, 0, 7, 0, 7, 0, 10, 0, 10, 0]\n",
            "['HH IH Z', '?', 'AE N D', '?', 'AE N D', '?', 'HH IH Z', '?', 'HH IH Z', '?']\n",
            "\n",
            "['And came into the vale in little space,']\n",
            "[7, 529, 36, 0, 31, 0, 31, 0, 31, 0]\n",
            "['AE N D', 'S T UH D', 'AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['In forest wild, in thicket, brake, or den;']\n",
            "[67, 0, 50, 0, 50, 0, 50, 0, 0, 0]\n",
            "['IH N', '?', 'AO R', '?', 'AO R', '?', 'AO R', '?', '?', '?']\n",
            "\n",
            "['In spite of changes. Look for me until']\n",
            "[62, 65, 47, 446, 7, 446, 7, 59, 7, 740]\n",
            "['F AO R', 'EH', 'V ER', 'N AW', 'AE N D', 'N AW', 'AE N D', 'M IY', 'AE N D', 'T IH L']\n",
            "\n",
            "['The parchment of the future, it were vain--']\n",
            "[31, 0, 0, 0, 1707, 0, 1707, 0, 156, 0]\n",
            "['DH AH', '?', '?', '?', 'IH T S', '?', 'IH T S', '?', 'W AA Z', '?']\n",
            "\n",
            "['The spirits of the Seasons seem to stand.']\n",
            "[31, 0, 31, 40, 31, 0, 31, 40, 31, 0]\n",
            "['DH AH', '?', 'DH AH', 'AH V', 'DH AH', '?', 'DH AH', 'AH V', 'DH AH', '?']\n",
            "\n",
            "['For lo! his passion, but an art of craft,']\n",
            "[62, 0, 31, 0, 34, 1370, 10, 425, 10, 0]\n",
            "['F AO R', '?', 'DH AH', '?', 'SH AH N', 'W ER K', 'HH IH Z', 'HH Y UW', 'HH IH Z', '?']\n",
            "\n",
            "['Is due, and not to war, intrepid still']\n",
            "[86, 0, 62, 0, 31, 0, 31, 0, 0, 0]\n",
            "['IH Z', '?', 'F AO R', '?', 'DH AH', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['Or sleeping, walk a restless world of dreams.']\n",
            "[50, 561, 36, 0, 40, 561, 31, 561, 36, 561]\n",
            "['AO R', 'S L IY P', 'AH', '?', 'AH V', 'S L IY P', 'DH AH', 'S L IY P', 'AH', 'S L IY P']\n",
            "\n",
            "['Of teaching school, a school of Czechs and Poles']\n",
            "[40, 0, 0, 0, 0, 0, 0, 0, 31, 0]\n",
            "['AH V', '?', '?', '?', '?', '?', '?', '?', 'DH AH', '?']\n",
            "\n",
            "['And drops of blood bedew the conscious lawn;']\n",
            "[7, 0, 40, 0, 40, 0, 31, 0, 31, 0]\n",
            "['AE N D', '?', 'AH V', '?', 'AH V', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['From daylight till the evening, wet or fine,']\n",
            "[67, 0, 31, 1078, 31, 0, 31, 1500, 50, 0]\n",
            "['IH N', '?', 'DH AH', 'K OW L D', 'DH AH', '?', 'DH AH', 'HH IY T', 'AO R', '?']\n",
            "\n",
            "['A doom that ever poised itself to fall,']\n",
            "[36, 0, 40, 0, 36, 0, 36, 0, 40, 90]\n",
            "['AH', '?', 'AH V', '?', 'AH', '?', 'AH', '?', 'AH V', 'HH EH L']\n",
            "\n",
            "['Of wailing women pierce the vaulted skies.']\n",
            "[40, 0, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['AH V', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['To happy havens under all the sky,']\n",
            "[30, 0, 36, 0, 31, 0, 31, 0, 31, 1257]\n",
            "['T UW', '?', 'AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', 'S K AY']\n",
            "\n",
            "['In purple clusters hangs the vines increase,']\n",
            "[31, 0, 522, 0, 31, 0, 31, 0, 522, 0]\n",
            "['DH AH', '?', 'AA R', '?', 'DH AH', '?', 'DH AH', '?', 'AA R', '?']\n",
            "\n",
            "['A shepherd meeting thee, the oar surveys,']\n",
            "[31, 0, 180, 0, 31, 0, 31, 0, 31, 0]\n",
            "['DH AH', '?', 'S AH N', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['The printed passion of the dream remains:--']\n",
            "[31, 0, 0, 0, 0, 0, 31, 0, 256, 0]\n",
            "['DH AH', '?', '?', '?', '?', '?', 'DH AH', '?', 'D IH', '?']\n",
            "\n",
            "['And piecemeal shares and maims the felon round.']\n",
            "[7, 0, 7, 0, 7, 0, 31, 0, 31, 0]\n",
            "['AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['A happy nation, and a happy king.']\n",
            "[36, 277, 1002, 1146, 339, 0, 10, 497, 1002, 454]\n",
            "['AH', 'P IY S', 'P IY', 'P R AW D', 'T R IY', '?', 'HH IH Z', 'HH AE', 'P IY', 'L AH V']\n",
            "\n",
            "['And Miscreator, makes and helps along']\n",
            "[7, 0, 7, 0, 7, 0, 7, 0, 7, 0]\n",
            "['AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'AE N D', '?']\n",
            "\n",
            "['The dashing waters when the air is still']\n",
            "[31, 1751, 40, 356, 31, 356, 31, 356, 31, 850]\n",
            "['DH AH', 'SH AO R', 'AH V', 'W AO', 'DH AH', 'W AO', 'DH AH', 'W AO', 'DH AH', 'F L OW']\n",
            "\n",
            "['And still its ice the freezing silence kept:']\n",
            "[52, 0, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['DH AE T', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['That shrink in misty mournfulness from sight,']\n",
            "[52, 0, 0, 1140, 263, 0, 31, 0, 0, 0]\n",
            "['DH AE T', '?', '?', 'SH AE', 'D OW', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['And secret seeds of envy, lay behind;']\n",
            "[7, 0, 31, 0, 0, 0, 31, 0, 0, 0]\n",
            "['AE N D', '?', 'DH AH', '?', '?', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['Of trees and crack of branches, common things,']\n",
            "[40, 0, 7, 600, 7, 0, 7, 0, 7, 600]\n",
            "['AH V', '?', 'AE N D', 'TH IH NG Z', 'AE N D', '?', 'AE N D', '?', 'AE N D', 'TH IH NG Z']\n",
            "\n",
            "['And pious tribute at her altars pay:']\n",
            "[7, 0, 170, 0, 0, 0, 170, 0, 0, 0]\n",
            "['AE N D', '?', 'HH ER', '?', '?', '?', 'HH ER', '?', '?', '?']\n",
            "\n",
            "['Courageous, and refreshed for future toil,']\n",
            "[195, 0, 7, 0, 7, 0, 7, 0, 30, 0]\n",
            "['R IH', '?', 'AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'T UW', '?']\n",
            "\n",
            "['Upon the wall and windows blots of gilt.']\n",
            "[36, 0, 31, 0, 0, 0, 0, 0, 7, 0]\n",
            "['AH', '?', 'DH AH', '?', '?', '?', '?', '?', 'AE N D', '?']\n",
            "\n",
            "['And lay upon the threshold like a log.']\n",
            "[7, 0, 36, 0, 31, 40, 31, 40, 31, 0]\n",
            "['AE N D', '?', 'AH', '?', 'DH AH', 'AH V', 'DH AH', 'AH V', 'DH AH', '?']\n",
            "\n",
            "['In synod unbenign; and taught the fixed']\n",
            "[76, 1219, 31, 573, 31, 7, 31, 0, 31, 0]\n",
            "['W IH DH', 'S IY V D', 'DH AH', 'F ER S T', 'DH AH', 'AE N D', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['And left him to his rest.  An end of Love.']\n",
            "[7, 295, 30, 0, 10, 295, 10, 0, 36, 454]\n",
            "['AE N D', 'L EH F T', 'T UW', '?', 'HH IH Z', 'L EH F T', 'HH IH Z', '?', 'AH', 'L AH V']\n",
            "\n",
            "['And human hearts and minds to show it by,']\n",
            "[7, 425, 31, 0, 40, 425, 40, 425, 426, 671]\n",
            "['AE N D', 'HH Y UW', 'DH AH', '?', 'AH V', 'HH Y UW', 'AH V', 'HH Y UW', 'M AH N', 'HH AA R T']\n",
            "\n",
            "['The outward struggle and the inward strife.']\n",
            "[31, 0, 40, 40, 31, 7, 31, 7, 40, 274]\n",
            "['DH AH', '?', 'AH V', 'AH V', 'DH AH', 'AE N D', 'DH AH', 'AE N D', 'AH V', 'S T R AY F']\n",
            "\n",
            "['His arms and bosom, seem to melt away.']\n",
            "[10, 0, 522, 0, 522, 0, 10, 0, 36, 0]\n",
            "['HH IH Z', '?', 'AA R', '?', 'AA R', '?', 'HH IH Z', '?', 'AH', '?']\n",
            "\n",
            "['And noise, and tumult rises from the crowd.']\n",
            "[7, 0, 36, 0, 31, 0, 31, 0, 31, 0]\n",
            "['AE N D', '?', 'AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['Cannot devise a torment, so it be']\n",
            "[399, 587, 102, 0, 878, 224, 36, 0, 36, 0]\n",
            "['IH F', 'N AA T', 'B IY', '?', 'K AE N', 'DH IH S', 'AH', '?', 'AH', '?']\n",
            "\n",
            "['The damsel put the pair without reply:']\n",
            "[31, 0, 0, 0, 31, 0, 31, 0, 195, 0]\n",
            "['DH AH', '?', '?', '?', 'DH AH', '?', 'DH AH', '?', 'R IH', '?']\n",
            "\n",
            "['Or that it hardens more and helps to bind']\n",
            "[50, 0, 231, 0, 30, 0, 30, 0, 30, 0]\n",
            "['AO R', '?', 'IH T', '?', 'T UW', '?', 'T UW', '?', 'T UW', '?']\n",
            "\n",
            "['And flowing waters, and the starry flame,']\n",
            "[7, 0, 31, 0, 31, 7, 31, 7, 31, 0]\n",
            "['AE N D', '?', 'DH AH', '?', 'DH AH', 'AE N D', 'DH AH', 'AE N D', 'DH AH', '?']\n",
            "\n",
            "['The spoil or spoiler? this behold in me;']\n",
            "[31, 0, 31, 0, 31, 40, 31, 0, 31, 0]\n",
            "['DH AH', '?', 'DH AH', '?', 'DH AH', 'AH V', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['And rode with them, and spake to none; the day']\n",
            "[7, 0, 7, 0, 7, 0, 7, 0, 7, 0]\n",
            "['AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'AE N D', '?', 'AE N D', '?']\n",
            "\n",
            "['Of finest metal was her armour bright,']\n",
            "[40, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['AH V', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
            "\n",
            "['The doleful city all around invest,']\n",
            "[31, 0, 0, 0, 31, 0, 31, 0, 0, 0]\n",
            "['DH AH', '?', '?', '?', 'DH AH', '?', 'DH AH', '?', '?', '?']\n",
            "\n",
            "['Of crowded cities, was from passing tale']\n",
            "[31, 0, 31, 460, 31, 0, 31, 460, 31, 0]\n",
            "['DH AH', '?', 'DH AH', 'S IH', 'DH AH', '?', 'DH AH', 'S IH', 'DH AH', '?']\n",
            "\n",
            "['An Arab fearless as the face of Truth']\n",
            "[36, 0, 40, 0, 0, 0, 36, 0, 40, 0]\n",
            "['AH', '?', 'AH V', '?', '?', '?', 'AH', '?', 'AH V', '?']\n",
            "\n",
            "['It lightened toil, and took the edge from pain,']\n",
            "[231, 0, 7, 0, 7, 0, 31, 0, 31, 0]\n",
            "['IH T', '?', 'AE N D', '?', 'AE N D', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['That brought with it another civil war.']\n",
            "[52, 0, 31, 0, 31, 0, 31, 460, 31, 0]\n",
            "['DH AE T', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', 'S IH', 'DH AH', '?']\n",
            "\n",
            "['Apart from all, in pride of conscious strength,']\n",
            "[40, 686, 28, 686, 31, 686, 31, 686, 31, 686]\n",
            "['AH V', 'S T R EH NG K TH', 'K W AH L', 'S T R EH NG K TH', 'DH AH', 'S T R EH NG K TH', 'DH AH', 'S T R EH NG K TH', 'DH AH', 'S T R EH NG K TH']\n",
            "\n",
            "['And far and near throughout the land the men']\n",
            "[7, 107, 31, 0, 31, 0, 31, 0, 31, 775]\n",
            "['AE N D', 'AA N', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', 'M EH N']\n",
            "\n",
            "['Remembered--I. And, oh! a little while']\n",
            "[36, 81, 18, 911, 36, 0, 18, 0, 7, 0]\n",
            "['AH', 'L IH', 'T AH L', 'W AY L', 'AH', '?', 'T AH L', '?', 'AE N D', '?']\n",
            "\n",
            "['And clip his borders short, and drive his herds,']\n",
            "[7, 0, 10, 0, 7, 0, 10, 0, 10, 0]\n",
            "['AE N D', '?', 'HH IH Z', '?', 'AE N D', '?', 'HH IH Z', '?', 'HH IH Z', '?']\n",
            "\n",
            "['And suffer me in anguish to depart']\n",
            "[7, 0, 36, 0, 40, 0, 40, 0, 256, 0]\n",
            "['AE N D', '?', 'AH', '?', 'AH V', '?', 'AH V', '?', 'D IH', '?']\n",
            "\n",
            "['The king from sovereign peril saved his head,']\n",
            "[31, 0, 0, 0, 31, 0, 31, 0, 31, 0]\n",
            "['DH AH', '?', '?', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['The point is turned; the twilight shadow fills']\n",
            "[31, 706, 31, 0, 31, 0, 31, 0, 31, 0]\n",
            "['DH AH', 'G EH N', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?', 'DH AH', '?']\n",
            "\n",
            "['Arriving all confused among the rest']\n",
            "[36, 1674, 7, 7, 7, 7, 7, 7, 36, 0]\n",
            "['AH', 'G EH', 'AE N D', 'AE N D', 'AE N D', 'AE N D', 'AE N D', 'AE N D', 'AH', '?']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yyDGVtigW57f",
        "colab_type": "code",
        "outputId": "3fcc71fb-a932-45d7-86d3-d4063c2d7c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "categories = df_train.label.cat.categories.tolist()\n",
        "predict_logits = predicts.argmax(axis=1)\n",
        "print(\"Categorie: {0}\".format(categories))\n",
        "predict_labels = [categories[logit] for logit in predict_logits]\n",
        "predict_labels"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7f6e6bf5580f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Categorie: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict_logits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hYhmukbSKpnp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "os.remove('./model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}