{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "co7MV6sX7Xto"
   },
   "source": [
    "# [Keras + Universal Sentence Encoder = Deep Meter] (https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAVQGidpL8v5"
   },
   "source": [
    "This notebook creates an autoencoder using the Universal Sentence Encoder. The autoencoder output is CMUdict syllables. The dataset is that subset of Allison Parrish's Project Gutenberg poetry archive which happens to scan in iambic pentameter.\n",
    "\n",
    "The notebook is based on Chengwei Zhang's example of wrapping the USE inside a larger tensorflow model saves to a Keras model (without save the USE itself in the TF model).\n",
    "\n",
    "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n",
    "\n",
    "Since there are 10 one-hot values for 10 sets of 6k syllables, this is \"multi-label classification\"\n",
    "Changes for multi-label classification:\n",
    "sigmoid activation instead of softmax\n",
    "binary_crossentropy\n",
    "\n",
    "Text format is tab-separated, 2 columns: first text, second multi-level\n",
    "array of syllables:\n",
    "\n",
    "Multi-output version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOTzp8O36CyQ"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "lVjNK8shFKOC",
    "outputId": "6d7ffdb7-c4d8-498a-fd97-bbd038b34c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep_meter' already exists and is not an empty directory.\n",
      "/home/ds/notebooks/content/deep_meter\n",
      "Already up to date.\n",
      "gzip: blobs/*.gz: No such file or directory\n",
      "gzip: prepped_data/*.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Install the latest Tensorflow version.\n",
    "#!pip3 install --quiet \"tensorflow>=1.7\"\n",
    "# Install TF-Hub.\n",
    "#!pip3 install --quiet tensorflow-hub\n",
    "#%cd /content\n",
    "!git clone https://github.com/LanceNorskog/deep_meter || true\n",
    "%cd deep_meter\n",
    "!git pull\n",
    "# could not figure out how to read gzipped files as text!\n",
    "!gunzip -qf blobs/*.gz || true\n",
    "!gunzip -qf prepped_data/*.gz || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSeY-MUQo2Ha"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# boilerplate from base notebook\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Nadam, Adam\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feBc_8Y-pt6F"
   },
   "outputs": [],
   "source": [
    "# github deep_meter code\n",
    "import utils\n",
    "# should not need this to use utils.flatten but is true anyway?\n",
    "from itertools import chain\n",
    "import subprocess\n",
    "import syllables\n",
    "# misc for this notebook\n",
    "from ast import literal_eval\n",
    "\n",
    "import scipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwty8Z6mAkdV"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q8F4LNGFqOiq",
    "outputId": "8d573b58-d482-48c1-c3d1-80aff1f6bb07"
   },
   "outputs": [],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "#embed = hub.Module(module_url)\n",
    "#embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwAQNy1eMDkQ"
   },
   "outputs": [],
   "source": [
    "# read classified poetry lines: text tab [['syll', 'la', 'ble'], ...]\n",
    "# clip to only most common syllables with syllable manager\n",
    "# ['words', ...], [[[0,0,1,0], ...]]\n",
    "def get_data(filename, syll_mgr, num_symbols):\n",
    "    num_syllables = syll_mgr.get_size()      \n",
    "    lines = open(filename, 'r').read().splitlines()\n",
    "    num_lines = len(lines)\n",
    "    num_lines = 30\n",
    "    text_lines = [None] * num_lines\n",
    "    label_array = [None] * num_symbols\n",
    "    for j in range(num_symbols):\n",
    "        label_array[j] = ([None] * num_lines)\n",
    "    for i in range(0, num_lines):\n",
    "      parts = lines[i].split(\"\\t\")\n",
    "      label = utils.flatten(literal_eval(parts[1]))\n",
    "      if len(label) != num_symbols:\n",
    "        continue\n",
    "      for j in range(num_symbols):\n",
    "        enc = np.zeros(syll_mgr.get_size(), dtype=np.int8)\n",
    "        enc[syll_mgr.get_encoding(label[j])] = 1\n",
    "        label_array[j][i] = enc\n",
    "      text_lines[i] = str([parts[0]])\n",
    "\n",
    "    return (text_lines, np.array(label_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HAtd4X5DayF"
   },
   "outputs": [],
   "source": [
    "# syllables in descending order of occurrence - 6k in gutenberg.iambic_pentameter, 15k total\n",
    "# clamp to most common 100 syllables while debugging- use NCE to get all syllables or interesting number\n",
    "# 98 + pause + wildcard\n",
    "# iambic pentameter\n",
    "num_symbols = 10\n",
    "#syll_mgr = syllables.syllables(num_syllables)\n",
    "syll_mgr = syllables.syllables()\n",
    "num_syllables = syll_mgr.get_size() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eN9aqig-QpDZ",
    "outputId": "976ad91a-add2-4946-b05e-5a89bae76889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_text, train_label) = get_data('prepped_data/gutenberg.iambic_pentameter.train', syll_mgr, num_symbols)\n",
    "num_training = len(train_text)\n",
    "#train_text = train_text[0:100]\n",
    "#train_label = train_label[0:100]\n",
    "\n",
    "(test_text, test_label) = get_data('prepped_data/gutenberg.iambic_pentameter.test', syll_mgr, num_symbols)\n",
    "#test_text = test_text[0:100]\n",
    "#test_label = test_label[0:100]\n",
    "\n",
    "num_testing = len(test_text)\n",
    "train_label.shape\n",
    "#test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sf9A4Xl6J7c6"
   },
   "source": [
    "## Embed training & test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected string, got None of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f030e6e395ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_text_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                          as_ref=False):\n\u001b[1;32m    227\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    205\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    206\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 207\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    208\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ds/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 353\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string, got None of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "# important?\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
    "\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [None] * 1\n",
    "messages[0] = str(train_text[0])\n",
    "\n",
    "mess_t = tf.convert_to_tensor(messages, dtype='string', name='dumb_test')\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(type(train_text))\n",
    "train_text_t = tf.convert_to_tensor(train_text, dtype='string', name='training_text')\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  print(session.run(embed(mess_t))[0][0:3])\n",
    "  train_embeddings = session.run(embed(train_text_t))\n",
    "  test_embeddings = session.run(embed(test_text))\n",
    "train_text_d = np.array(train_embeddings)\n",
    "test_text_d = np.array(test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mQUV_7dIm-9"
   },
   "outputs": [],
   "source": [
    "print(test_text_d.shape)\n",
    "# slow\n",
    "num_epochs = 2\n",
    "adam_0001 = Adam(0.0001)\n",
    "adam_0001 = tf.contrib.opt.NadamOptimizer(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qX2rBOuxDP1m"
   },
   "source": [
    "## Assemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "t3fllZkVjXKV",
    "outputId": "16dd63fb-37a3-4d3b-950d-e1d0a2e23413"
   },
   "outputs": [],
   "source": [
    "input_embeddings = layers.Input(shape=(512,), dtype=tf.float32, name='Input')\n",
    "dropout_input = layers.Dropout(0.2)(input_embeddings)\n",
    "dense = layers.Dense(768, activation='relu', name='Convoluted')(dropout_input)\n",
    "pred_array = []\n",
    "loss_array = []\n",
    "names_array = []\n",
    "for i in range(num_symbols):\n",
    "  name = 'Flatout'+str(i)\n",
    "  pred_array.append(layers.Dense(num_syllables, activation='sigmoid', name=name)(dense))\n",
    "  loss_array.append('categorical_crossentropy')\n",
    "  names_array.append(name)\n",
    "model = Model(inputs=input_embeddings, outputs=pred_array)\n",
    "model.compile(loss=loss_array, \n",
    "              optimizer=adam_0001, \n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqcRy_JWXe0u"
   },
   "source": [
    "## Train Keras model and save weights\n",
    "This only trains and save our Keras layers not the embed module' weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "_stfC_7VFhS8",
    "outputId": "ae376911-2ba8-4519-cd6d-5e2a20f97254"
   },
   "outputs": [],
   "source": [
    "use_saved_model=False\n",
    "if not use_saved_model or not os.path.exists('./model.h5'):\n",
    "  with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    train_labels = [None] * len(train_label)\n",
    "    test_labels = [None] * len(test_label)\n",
    "    for i in range(num_symbols):\n",
    "        train_labels[i] = train_label[i]\n",
    "        test_labels[i] = test_label[i]\n",
    "    history = model.fit(train_text_d, \n",
    "            train_labels,\n",
    "            validation_data=(test_text_d, test_labels),\n",
    "            epochs=num_epochs,\n",
    "            callbacks = [EarlyStopping(patience=2)],\n",
    "            batch_size=32,\n",
    "            verbose=2)\n",
    "    model.save_weights('./model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UW1CiBhnXnxa",
    "outputId": "8ff5ed9f-56dd-4b3f-a866-9c141634afdd"
   },
   "outputs": [],
   "source": [
    "!ls -alh | grep model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQux6qLdXabG"
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSDxetlfUEiD"
   },
   "outputs": [],
   "source": [
    "\n",
    "#new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  model.load_weights('./model.h5')  \n",
    "  predicts = model.predict(test_text_d, batch_size=32)\n",
    "\n",
    "print(predicts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "U9V5JOWGq4Cd",
    "outputId": "c627b01f-22c6-42b8-df31-a1dfddb6e124"
   },
   "outputs": [],
   "source": [
    "first = predicts[0]\n",
    "max_d = -1000000\n",
    "min_d = 1000000\n",
    "max_i = 0\n",
    "min_i = 0\n",
    "for i in range(len(predicts) - 1):\n",
    "  v = predicts[i]\n",
    "  d = scipy.spatial.distance.cosine(v, first)\n",
    "  if max_d < d:\n",
    "    max_d = d\n",
    "    max_i = i\n",
    "  if min_d > d:\n",
    "    min_d = d\n",
    "    min_i = i\n",
    "print(\"Min and max distances: {0} and {1}\".format(min_d, max_d))\n",
    "print(test_text[max_i])\n",
    "syll_mgr.interpret2(predicts[max_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6137
    },
    "colab_type": "code",
    "id": "xaR0d5VPU23Z",
    "outputId": "c54f3c53-39cf-41b2-a08f-6f31c825c223"
   },
   "outputs": [],
   "source": [
    "for i in range(10,100):\n",
    "  print(test_text[i])\n",
    "  syll_mgr.interpret2(predicts[i])\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "yyDGVtigW57f",
    "outputId": "f7620c4b-01dc-4b67-9874-e3c349f81f45"
   },
   "outputs": [],
   "source": [
    "categories = df_train.label.cat.categories.tolist()\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "print(\"Categorie: {0}\".format(categories))\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYhmukbSKpnp"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.remove('./model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Meter: Keras + Universal Sentence Encoder + CMUdict + Project Gutenberg poetry archives",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
